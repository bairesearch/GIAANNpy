DONE//do:
o1 preview prompt 1b2d;

v1b2d:
Please create an abstract biological neural neural network simulation for natural language processing in python. This is the specification:
- Use the pytorch API for hardware acceleration of all arrays (for efficient parallel processing of connectivity). Do not initialise the array with a predetermined dictionary; dynamically populate the arrays as new dictionary words and relations are encountered. Will be discussed below.
- Use vector graphics library such as networkx to always maintain a complete visualisation of the network (including active neurons). Only draw the parts of the network that are not empty.
- The application continuously reads sequences (i.e. sentences) from a large textual corpus from huggingface (using a streamed Wikipedia dataset). Please use the library nltk.
- It identifies all possible POS (parts of speech) tags of each word in a sequence. Some words may have more than one possible POS tag. Please use the library spacy. 
- Perform lemmatisation of each word in sentence before processing it in the neural network. Use a suitable python library, e.g. spacy. Assign concept columns to lemmas not words. Assign feature neurons to words not lemmas.
- Maintain a boolean (useInference), and disable useInference mode. Maintain a boolean (lowMem), and enable lowMem mode. lowMem can only be used when useInference is disabled. It constrains the memory/computational processing of the network. 
- Maintain a boolean (usePOS), enable usePOS mode. If usePOS is enabled, a validity check of every lemma/word processed is performed to check its POS value. We will only assign unique concept columns for nouns that exist in this dictionary (always check POS). We will only assign feature neurons for non-nouns in this dictionary (always check POS). Nouns and non-nouns are detected using an English dictionary. Obtain lists of nouns and non-nouns using the nltk wordnet library.
- Every valid unique sequence lemma processed in the dataset is assigned a unique column in the network. Add these lemmas as keys in a concept columns dictionary (of dynamic size c). If usePOS is enabled, the validity check is peformed to check if the lemma is a noun before assigning a unique concept column. If usePOS is enabled and lowMem is disabled, set the size of the feature arrays (f) to the maximum number of non-nouns in an English dictionary. Otherwise, set the size of the feature arrays (f) to the current number of concept columns (c).
- Every valid unique sequence lemma processed in the current sequence (not dataset) is assigned an observed column object. Add the lemmas as keys in an observed columns dictionary. Add the observed columns objects as values in this observed columns dictionary. The observed columns dictionary along with its observed column objects are refreshed every time a new sequence is processed, however the dataset concept columns dictionary is maintained. 
- Create a class defining observed columns. The observed column class contains an index to the dataset concept column dictionary. The observed column class contains a list of feature connection arrays. The observed column class also contains a list of feature neuron arrays when useLowMem mode is enabled. The reason connection data must be stored in observed columns is because they are large arrays and every column's connections cannot be stored in RAM simultaneously.
- If lowMem is enabled, the observed columns contain a list of arrays (pytorch) of f feature neurons, where f is the maximum number of feature neurons per column. If lowMem is disabled, create a global list of 2D arrays (pytorch) of c * f feature neurons, where c is the current number of concept columns, and where f is the maximum number of feature neurons per column. Create arrays for neuron strength, neuron permanence, and neuron activation (added to the list). The first feature neuron in the feature array (for each column) is always the concept neuron of the column (regardless of its name or POS). Note, feature neurons are combined in various combinations to represent different instances of a concept (or a specific concept thereof).
- Each column represents a unique concept. Columns (lemmas) can also represent similar concepts (synonyms), but we will ignore semantically similar words for now. We will also ignore the fact words can have multiple meanings for now (i.e. words can represent independent concepts). In this case particular columns will be semantically overloaded (polysemes/homographs).
- If usePOS is disabled, and if a column detects too many unique proximal features during training, it is likely that the column represents a common word (such as a definite/indefinite article) and will either a) be discarded or b) will not reliably contribute to next word predictions (as a consequence of the way in which connection weights are normalised by the number of feature neurons in a column). After training, the only columns to perform reliable predictions will be those representing concepts (typically substances/nouns). 
- The columns represent general concepts (via the activation of their concept neuron), but they can also represent more specific concepts or instances by incorporating/activating feature neurons. Feature neurons are used to define instances of concepts. There are different classes of feature neurons, including relations such as actions/verbs and conditions/prepositions, qualities/adjectives, or modifiers/adverbs, etc. We are will not be concerned about the specific classes of feature neurons for now; their distinct connectivity should emerge out of the training dynamics of the network.
- The column concepts typically represent substances (nouns), but they can also represent actions or conditions (verbs or prepositions). When usePOS is enabled, column concepts are restricted to substances (nouns). When usePOS is disabled, they assigned for every new lemma encountered in the dataset sequence. 
- For every concept word (lemma) i in the sequence, identify every feature neuron in that column that occurs q words before or after the concept word in the sequence, including the concept neuron. Note that this is similar to the bag of words algorithm. Increment the corresponding (i.e. contextual) elements (q) of the strength feature neuron array of each sequence word (i) observed concept column object. If a feature neuron has been encountered by a concept column its integer strength value will still be incremented.
- Create column internal and external connections. Connect these feature neurons to each other. Connect these feature neurons to every other identified feature neuron (observed in the current sequence) in every other concept column in the sequence. Connections are made by incrementing an integer strength value in a connection array (initialised as 0). If a connection has already been made this integer strength value will still be incremented.
- Store all connections for each source column in a list of integer 3D feature connection arrays, each of size f * c * f, where c is the length of the dictionary of columns, and f is the maximum number of feature neurons. The list of feature connection arrays includes a connection strength array (pytorch), connection permanence array, and connection activation array. The direction of a connection created corresponds to the word order of the features in the sequence. The lists of connection arrays are stored in the observed column class objects when in RAM.
- If usePOS is disabled, set q to 5. If usePOS is enabled, set q to the distance to the previous/next noun - 1 (depending on whether the feature selected is before or after the current concept word in the sequence). Always ensure the feature neuron selected is not out of bounds of the sequence. The closer the words occur in a sentence, the more likely they are to be connected, but this is not always the case. Consider an example of an exception; "the cat that ate the mouse ran to the park": in this example "ran" and "ate" are directly connected to "cat", "ran" is not directly connected to "mouse" despite being nearest to it.
- You should be aware this this will generate a large set of word associations for each concept word similar to historic NLP algorithms like the word co-occurance matrix.
- Generate all necessary code to run the abstract biological neural network simulation. Visualise the complete graph every time a new sentence is parsed by the application.
- The columns should be visualised along the horizontal x axis (they should each be a tall rectangle). Each concept word neuron should be stored at the bottom of each column. The other feature neurons should be stored along the vertical y axis of each column, above the concept neurons. Draw the connections between all feature neurons and their targets.  
- Please draw separate rectangles for each concept column. The concept column rectangles need to be drawn around the contents (concept and feature neurons) of each column. Ensure the neuron sizes are too large to be encapsulated by the rectangles and all the neurons (including the relation neurons) are drawin within the column rectangles.
- Print each sentence in the command line.
- Draw the concept neurons blue, and all other feature neurons in cyan.
- Draw all column internal feature connections in yellow, and all column external feature connections in orange. Remember that column internal feature connections and column external feature connections can be identified by their index in the connections array.
- Observed column object data are brought in/out of RAM from disk on demand. They are brought into RAM from disk for every column word in the sequence, and are saved back to disk after processing every sequence. This includes all array data within the list of feature connection arrays (and all array data within the list of feature neurons arrays if lowMem mode is enabled). If lowMem mode is enabled, global feature neurons arrays are brought in/out of RAM from disk before/after processing every sequence.
- The observed column object data and arrays are only brought into RAM after all new unique columns have been detected in the current sequence. These are added to the dataset concept columns dictionary. When the arrays are brought into RAM; if the concept columns dictionary has increased since the processing of the last sequence, the array rows will be expanded to accommodate the new concept columns (with all new rows properly initialised to their default value). If usePOS mode is disabled, the connection array columns will also be expanded to accommodate the new concept columns.
- The concept columns dictionary is also saved to disk after every sentence. When useInference is enabled, the concept columns dictionary is loaded from disk during startup, before processing the first sequence in the dataset.
- When writing the code, add the specification text to comments.
- When writing the code, always add complete code for all modes; do not skip code for disabled modes.
- Place the main processing loop in a separate function (i.e. "for article in dataset:"). Likewise, place every for loop and if statement within the second level of the main processing loop in a separate function (i.e. place every for loop and if statement at the second lowest nesting depth, directly within "for sentence in sentences:" in a separate function).
- Every feature neuron in a concept column has an integer permanence value (initialised as z1=3). A feature neuron's permanence integer is increased by a function zf every time the feature column neuron is activated by a sequence with that particular feature neuron being activated. A feature neuron's permanence integer is decreased by z2 (set to 1) every time the concept column neuron is activated by a sentence without that particular feature neuron being activated. If a feature neuron's permanence decreases to 0, then the feature neuron is removed from the column and it will no longer be visualised. 
- Likewise, store a permanence value for every feature connection, using the same permanence rules used for feature neurons.
- Every time a feature neuron or connection's permanence is increased (assuming the feature neuron or connection already exists), it exponentially increases its permanence from its current value (via the function zf). Set zf to a squared function of its current integer permanence value. All permanence values are intialised to z1 (3), but do not square them when they are first initialised. Permanence still decreases linearly every time a concept column is activated by the current word but the connection or neuron within it is not referenced by the sequence (-1). The higher the permanence the stronger the weight for next word prediction. We are ignoring next word prediction for now, which involves an iterative topk selection of most activated neurons/columns (similar to hopfield networks).
- Set an integer activation trace for every neuron and connection in the network. A feature neuron or connection is temporarily excited (set an activation trace value of +1) every time it is referenced by a word in a sequence. This temporary excitation lasts for j1=5 sequences before being removed (set an activation trace value of 0), unless it is activated by another sequence in the meantime. If it is activated by another sequence in the meantime, its activation trace is not modified (keep an activation trace value of +1), but overwrite this value in the code (set an activation trace value of +1) in case an implementation change is introduced in the future. The higher the activation trace the stronger the weight for next word prediction. We are not implementing next word prediction yet.
FUTURE; - Create a wrapper function called prediction for when useInference mode is enabled, and a wrapper function called training for when useInference mode is disabled. Prediction is similar to training, however the neuron/connection strengths and permanences are never modified (only the activation trace is modified). After processing x seed words in the sequence data (where x=5), the network continues to predict the next most active concept columns in the network. It compares its topk column predictions (set k=1) to the actual words in the sequence data. The observed column object data (i.e. list of feature connection arrays) for the topk predicted columns are brought into RAM after each topk column prediction, and removed from RAM after the next set of topk column predictions. When a concept column has been predicted, the connections of all of its active feature neurons are then activated, to calculate the next topk column predictions. If a concept column has previously been activated in the dataset, its activation trace may positively bias the current topk column selection (depending on the length of time from its last activation). 
CHATONLY/MANUAL; - If usePOS is disabled, set q to 5. If usePOS is enabled, set q to the distance to the previous/next noun - 1 (depending on whether the feature selected is before or after the current concept word in the sequence). Always ensure the feature neuron selected is not out of bounds of the sequence.
CHATONLY; - Expand all the observed column feature connection arrays by 1 dimension (from 2D to 3D) to be each of size f * c * f, where c is the length of the dictionary of columns, and f is the maximum number of feature neurons per column. Update the existing feature connection code; feature neurons are connected to every other identified feature neuron (observed in the current sequence) in every other concept column in the sequence.
MANUAL; - If usePOS is enabled and lowMem is disabled, set the size of the feature arrays (f) to the maximum number of non-nouns in an English dictionary. Otherwise, set the size of the feature arrays (f) to the current number of concept columns (c).
MANUAL; - Create the concept neuron in the feature array of the observed column.

