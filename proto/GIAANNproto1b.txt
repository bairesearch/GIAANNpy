DONE//do:
o1 preview prompt 1b13b;

v1b13b:
Please create an abstract biological neural neural network simulation for natural language processing in python. This is the specification:
- Use the pytorch API for hardware acceleration of all arrays (for efficient parallel processing of connectivity). Do not initialise the array with a predetermined dictionary; dynamically populate the arrays as new dictionary words and relations are encountered. Will be discussed below.
- Use vector graphics library such as networkx to always maintain a complete visualisation of the network (including active neurons). Only draw the parts of the network that are not empty.
- The application continuously reads sequences (i.e. sentences) from a large textual corpus from huggingface (using a streamed Wikipedia dataset). Please use the library nltk.
- It identifies all possible POS (parts of speech) tags of each word in a sequence. Some words may have more than one possible POS tag. Please use the library spacy. 
- Perform lemmatisation of each word in sentence before processing it in the neural network. Use a suitable python library, e.g. spacy. Assign concept columns to lemmas not words. Assign feature neurons to words not lemmas.
- Maintain a boolean (useInference), and disable useInference mode. Maintain a boolean (lowMem), and enable lowMem mode. lowMem can only be used when useInference is disabled. It constrains the memory/computational processing of the network. 
- Maintain a boolean (usePOS), enable usePOS mode. If usePOS is enabled, a validity check of every lemma/word processed is performed to check its POS value. We will only assign unique concept columns for nouns that exist in this dictionary (always check POS). We will only assign feature neurons for non-nouns in this dictionary (always check POS). Nouns and non-nouns are detected using an English dictionary. Obtain lists of nouns and non-nouns using the nltk wordnet library.
- Every valid unique sequence lemma processed in the dataset is assigned a unique column in the network. Add these lemmas as keys in a concept columns dictionary (of dynamic size c). If usePOS is enabled, the validity check is peformed to check if the lemma is a noun before assigning a unique concept column. If usePOS is enabled and lowMem is disabled, set the size of the feature arrays (f) to the maximum number of non-nouns in an English dictionary.
- Every valid unique sequence lemma processed in the current sequence (not dataset) is assigned an observed column object. Add the lemmas as keys in an observed columns dictionary. Add the observed columns objects as values in this observed columns dictionary. The observed columns dictionary along with its observed column objects are refreshed every time a new sequence is processed, however the dataset concept columns dictionary is maintained. 
- Create a class defining observed columns. The observed column class contains an index to the dataset concept column dictionary. The observed column class contains a list of feature connection arrays. The observed column class also contains a list of feature neuron arrays when useLowMem mode is enabled. The reason connection data must be stored in observed columns is because they are large arrays and every column's connections cannot be stored in RAM simultaneously.
- If lowMem is enabled, the observed columns contain a list of arrays (pytorch) of f feature neurons, where f is the maximum number of feature neurons per column. If lowMem is disabled, create a global list of 2D arrays (pytorch) of c * f feature neurons, where c is the current number of concept columns, and where f is the maximum number of feature neurons per column. Create arrays for neuron strength, neuron permanence, and neuron activation (added to the list). The first feature neuron in the feature array (for each column) is always the concept neuron of the column (regardless of its name or POS). Note, feature neurons are combined in various combinations to represent different instances of a concept (or a specific concept thereof).
- Each column represents a unique concept. Columns (lemmas) can also represent similar concepts (synonyms), but we will ignore semantically similar words for now. We will also ignore the fact words can have multiple meanings for now (i.e. words can represent independent concepts). In this case particular columns will be semantically overloaded (polysemes/homographs).
- If usePOS is disabled, and if a column detects too many unique proximal features during training, it is likely that the column represents a common word (such as a definite/indefinite article) and will either a) be discarded or b) will not reliably contribute to next word predictions (as a consequence of the way in which connection weights are normalised by the number of feature neurons in a column). After training, the only columns to perform reliable predictions will be those representing concepts (typically substances/nouns). 
- The columns represent general concepts (via the activation of their concept neuron), but they can also represent more specific concepts or instances by incorporating/activating feature neurons. Feature neurons are used to define instances of concepts. There are different classes of feature neurons, including relations such as actions/verbs and conditions/prepositions, qualities/adjectives, or modifiers/adverbs, etc. We are will not be concerned about the specific classes of feature neurons for now; their distinct connectivity should emerge out of the training dynamics of the network.
- The column concepts typically represent substances (nouns), but they can also represent actions or conditions (verbs or prepositions). When usePOS is enabled, column concepts are restricted to substances (nouns). When usePOS is disabled, they assigned for every new lemma encountered in the dataset sequence. 
- For every concept word (lemma) i in the sequence, identify every feature neuron in that column that occurs q words before or after the concept word in the sequence, including the concept neuron. Note that this is similar to the bag of words algorithm. Increment the corresponding (i.e. contextual) elements (q) of the strength feature neuron array of each sequence word (i) observed concept column object. If a feature neuron has been encountered by a concept column its integer strength value will still be incremented.
- Create column internal and external connections. Connect these feature neurons to each other. Connect these feature neurons to every other identified feature neuron (observed in the current sequence) in every other concept column in the sequence. Connections are made by incrementing an integer strength value in a connection array (initialised as 0). If a connection has already been made this integer strength value will still be incremented.
- Store all connections for each source column in a list of integer 3D feature connection arrays, each of size f * c * f, where c is the length of the dictionary of columns, and f is the maximum number of feature neurons. The list of feature connection arrays includes a connection strength array (pytorch), connection permanence array, and connection activation array. The direction of a connection created corresponds to the word order of the features in the sequence. The lists of connection arrays are stored in the observed column class objects when in RAM.
- If usePOS is disabled, set q to 5. If usePOS is enabled, set q to the distance to the previous/next noun - 1 (depending on whether the feature selected is before or after the current concept word in the sequence). Always ensure the feature neuron selected is not out of bounds of the sequence. The closer the words occur in a sentence, the more likely they are to be connected, but this is not always the case. Consider an example of an exception; "the cat that ate the mouse ran to the park": in this example "ran" and "ate" are directly connected to "cat", "ran" is not directly connected to "mouse" despite being nearest to it.
- You should be aware this this will generate a large set of word associations for each concept word similar to historic NLP algorithms like the word co-occurance matrix.
- Generate all necessary code to run the abstract biological neural network simulation. Visualise the complete graph every time a new sentence is parsed by the application.
- The columns should be visualised along the horizontal x axis (they should each be a tall rectangle). Each concept word neuron should be stored at the bottom of each column. The other feature neurons should be stored along the vertical y axis of each column, above the concept neurons. Draw the connections between all feature neurons and their targets.  
- Please draw separate rectangles for each concept column. The concept column rectangles need to be drawn around the contents (concept and feature neurons) of each column. Ensure the neuron sizes are too large to be encapsulated by the rectangles and all the neurons (including the relation neurons) are drawin within the column rectangles.
- Print each sentence in the command line.
- Draw the concept neurons blue, and all other feature neurons in cyan.
- Draw all column internal feature connections in yellow, and all column external feature connections in orange. Remember that column internal feature connections and column external feature connections can be identified by their index in the connections array.
- Observed column object data are brought in/out of RAM from disk on demand. They are brought into RAM from disk for every column word in the sequence, and are saved back to disk after processing every sequence. This includes all array data within the list of feature connection arrays (and all array data within the list of feature neurons arrays if lowMem mode is enabled). If lowMem mode is enabled, global feature neurons arrays are brought in/out of RAM from disk before/after processing every sequence.
- The observed column object data and arrays are only brought into RAM after all new unique columns have been detected in the current sequence. These are added to the dataset concept columns dictionary. When the arrays are brought into RAM; if the concept columns dictionary has increased since the processing of the last sequence, the array rows will be expanded to accommodate the new concept columns (with all new rows properly initialised to their default value). If usePOS mode is disabled, the connection array columns will also be expanded to accommodate the new concept columns.
- The concept columns dictionary is also saved to disk after every sentence. When useInference is enabled, the concept columns dictionary is loaded from disk during startup, before processing the first sequence in the dataset.
- When writing the code, add the specification text to comments.
- When writing the code, always add complete code for all modes; do not skip code for disabled modes.
- Place the main processing loop in a separate function (i.e. "for article in dataset:"). Likewise, place every for loop and if statement within the second level of the main processing loop in a separate function (i.e. place every for loop and if statement at the second lowest nesting depth, directly within "for sentence in sentences:" in a separate function).
- Every feature neuron in a concept column has an integer permanence value (initialised as z1=3). A feature neuron's permanence integer is increased by a function zf every time the feature column neuron is activated by a sequence with that particular feature neuron being activated. A feature neuron's permanence integer is decreased by z2 (set to 1) every time the concept column neuron is activated by a sentence without that particular feature neuron being activated. If a feature neuron's permanence decreases to 0, then the feature neuron is removed from the column and it will no longer be visualised. 
- Likewise, store a permanence value for every feature connection, using the same permanence rules used for feature neurons.
- Every time a feature neuron or connection's permanence is increased (assuming the feature neuron or connection already exists), it exponentially increases its permanence from its current value (via the function zf). Set zf to a squared function of its current integer permanence value. All permanence values are intialised to z1 (3), but do not square them when they are first initialised. Permanence still decreases linearly every time a concept column is activated by the current word but the connection or neuron within it is not referenced by the sequence (-1). The higher the permanence the stronger the weight for next word prediction. We are ignoring next word prediction for now, which involves an iterative topk selection of most activated neurons/columns (similar to hopfield networks).
- Set an integer activation trace for every neuron and connection in the network. A feature neuron or connection is temporarily excited (set an activation trace value of +1) every time it is referenced by a word in a sequence. This temporary excitation lasts for j1=5 sequences before being removed (set an activation trace value of 0), unless it is activated by another sequence in the meantime. If it is activated by another sequence in the meantime, its activation trace is not modified (keep an activation trace value of +1), but overwrite this value in the code (set an activation trace value of +1) in case an implementation change is introduced in the future. The higher the activation trace the stronger the weight for next word prediction. We are not implementing next word prediction yet.
FUTURE; - Create a wrapper function called prediction for when useInference mode is enabled, and a wrapper function called training for when useInference mode is disabled. Prediction is similar to training, however the neuron/connection strengths and permanences are never modified (only the activation trace is modified). After processing x seed words in the sequence data (where x=5), the network continues to predict the next most active concept columns in the network. It compares its topk column predictions (set k=1) to the actual words in the sequence data. The observed column object data (i.e. list of feature connection arrays) for the topk predicted columns are brought into RAM after each topk column prediction, and removed from RAM after the next set of topk column predictions. When a concept column has been predicted, the connections of all of its active feature neurons are then activated, to calculate the next topk column predictions. If a concept column has previously been activated in the dataset, its activation trace may positively bias the current topk column selection (depending on the length of time from its last activation). 
CHATONLY/MANUAL; - If usePOS is disabled, set q to 5. If usePOS is enabled, set q to the distance to the previous/next noun - 1 (depending on whether the feature selected is before or after the current concept word in the sequence). Always ensure the feature neuron selected is not out of bounds of the sequence.
CHATONLY; - Expand all the observed column feature connection arrays by 1 dimension (from 2D to 3D) to be each of size f * c * f, where c is the length of the dictionary of columns, and f is the maximum number of feature neurons per column. Update the existing feature connection code; feature neurons are connected to every other identified feature neuron (observed in the current sequence) in every other concept column in the sequence.
MANUAL; - If usePOS is enabled and lowMem is disabled, set the size of the feature arrays (f) to the maximum number of non-nouns in an English dictionary.
MANUAL; - Create the concept neuron in the feature array of the observed column.
/MANUAL; - Do not update f and expand the observed column and global feature arrays after every new feature is detected by an observed column (e.g. process_concept_words:process_feature:expand_feature_arrays). Instead, update f and expand the observed column and global feature arrays prior to processing the concept words (e.g. process_concept_words). Create a dedicated function to detect all possible new features in the sequence, and update f and expand the observed column and global feature arrays accordingly. Update the arrays just once for a given sequence, instead of updating them incrementally for every new feature detected in the sequence. All possible new features in the sequence can be detected simply by searching for all new words (or non-nouns when in usePOS mode) in the sequence.
/MANUAL; - Only draw column feature neurons and connections if their strength is > 0, and their permanence is > 0.
MANUAL; - Maintain a concept features dictionary (of every feature name in the columns). The concept features dictionary is also saved to disk after every sentence. When useInference is enabled, the concept features dictionary is loaded from disk during startup, before processing the first sequence in the dataset.
MANUAL; - Increment the strength of the feature concept neurons
MANUAL; - Only connect column features to future or present (not past) column features. Create connections for concept neurons. 
MANUAL; - Decrease permanence of inactive connections between observed columns (not all columns). Do not remove feature neurons from columns (just set their permanence to 0). Decrease permanence of inactive connections in the same column. Add method; decrease permanence of inactive connections for activated features in column (instead of connections from inactive feature neurons in column). Move all the decrease permanence code to a separate function.
CHATONLY; - Replace all connections modification code in the functions decrease_permanence and process_feature with parallelised implementations (by multiplying/adding/subtracting their pytorch arrays instead of serially modifying their connections). All of the feature and connection arrays in the observed columns have the same number of dimensions.
CHATONLY; - Introduce sequence observed columns object arrays to improve the efficiency of training. Create a sequence observed columns class (SequenceObservedColumns) that contains sequence observed columns object arrays which stack a feature subset of the observed columns object arrays for the current sequence. Sequence observed columns object arrays are created for each feature neuron and feature connection array type currently stored within the observed columns class objects for the current sequence. The sequence observed columns object arrays only contain the features identified for the sequence (not every column feature, as is the case with the observed columns object arrays). The feature neuron arrays in the sequence observed columns class are of 2D shape cs * fs, where cs is the number of concept columns in the sequence, and fs is the number of unique features in the sequence. The feature connection arrays in the sequence observed columns class are of 4D shape cs * fs * cs * fs. The first dimension of the sequence observed columns arrays corresponds to observed column index in the sequence being processed. All other dimensions are similar to those in the original observed column object arrays (except that they contain sequence subsets cs and fs (instead of using c and f). Create a dictionary in the sequence observed columns object which maps the concept name corresponding to each observed column stored in a sequence observed columns array (key) to its index in the array (value). Populate the observed columns objects with their array data when loading the observed columns (creation of new concept column or reading from disk), as currently implemented. Once every observed column for a sequence has been loaded the sequence observed column arrays are then populated. Converted all sentence processing code to use the sequence observed columns object arrays (instead of observed columns object arrays), and the code should these arrays by passing the sequence observed columns object between functions. When the sentence has finished processing the sequence observed columns object arrays are then used to repopulate the observed columns object arrays. The observed columns object arrays are then saved (writing to disk), as currently implemented.
CHATONLY; - Parallelise all SequenceObservedColumns class operations (populate_arrays/update_observed_columns). Parallelise these serial operations by using appropriate mask arrays. Instead of using for loops, use array operations.
CHATONLY; - Parallelise the execution of process_feature. Instead of using the for loop, use pytorch array operations.
MANUAL; - Parallelise the execution of process_concept_words. Instead of using the for loop, use pytorch array operations.
MANUAL; - Use spacy instead of nltk to parse sentences. Add useDedicatedConceptNames - same word can have different pos making it classed as an instance feature or concept feature. Upgrade process_features to prioritise concept feature detection over instance feature detection.
MANUAL; - Patch process_features:feature_connections_active - only add connections for sequence activated neurons.
MANUAL; - Add dimensions to neurons and connections arrays; properties (strength, permanence, activation, time etc), type (all; with provision for action, condition, quality, modifier, concept etc). Collapse strength, permanence, activation arrays into single array (properties dim).
MANUAL; - Add sequenceObservedColumnsUseSequenceFeaturesOnly - sequence observed columns arrays only store sequence features. Add drawSequenceObservedColumns - draw sequence observed columns (instead of complete observed columns).
MANUAL; - Add useDedicatedConceptNames2 - Rename each concept neuron in observed columns/sequence observed columns to a replacement name variableConceptNeuronFeature, so that each observed column feature array has exactly the same set of features. Add SequenceObservedColumn.observed_columns_dict and SequenceObservedColumn.observed_columns_sequence_word_index_dict. Add ObservedColumn.concept_name and ObservedColumn.concept_sequence_word_index.
MANUAL; - Ensure word order is maintained for internal and external feature connections. Ensure word order is maintained for connections between columns (does not support multiple same concepts in same sentence). Ensure identical feature nodes are not connected together. Change networkx graph to directed graph. Add randomiseColumnFeatureXposition - shuffle x position of column internal features such that their connections can be better visualised.
MANUAL; - Set neuron/connection permanence increase function zf to be linear (+z1).
MANUAL; - Even when not in inference mode, load concept_columns_dict_file/concept_features_dict from database. Prepare non low memory for inference. Generalise graph draw code for all modes.
MANUAL; - Start inference mode. Update neuron activation to be cumulative (but reset upon dataset/database load). Update neuron connection activation to immediately add activation to their target nodes. Add SequenceObservedColumns.concept_indices_in_observed_list. Extract SequenceObservedColumns.identifyObservedColumnFeatureWords and SequenceObservedColumns.getObservedColumnFeatureIndices from SequenceObservedColumns.getObservedColumnFeatureWords.
MANUAL; - Start topk feature selection within topk columns.
MANUAL; - Add option sequenceObservedColumnsMatchSequenceWords: update sequence observed column arrays to store columns and features in order of sentence word index. Upgrade process features; prefer closer than further target neurons when strengthening connections or activating target neurons in sentence. Upgrade process features; add target neuron activation dependence on connection strength. Upgrade process features with option deactivateNeuronsUponPrediction; suppress neuron activations once processed. During inference, for each prediction predict both next word and column (same/diff to existing column). During inference, load an inference prompt (seed sentences) from disk. Remove useDedicatedConceptNames1. Store word.lower() in words list. Extract columns_index_sequence_word_index_dict from observed_columns_sequence_word_index_dict.
MANUAL; - Patch process_column_prediction. Extract process_features_active_train/predict from process_features_active; predict all targets in global feature arrays. Patch process_features_active; feature_connections_activation_update = feature_connections_active * feature_connections[array_index_properties_strength].
MANUAL; - Upgrade inference; once a concept column has been activated and its feature neurons have fired (connection targets have been activated), prevent them from firing again until a new concept column is activated. Upgrade inference seed phase to activate global target neurons and not train (process_column_inference_seed).
MANUAL; - Upgrade inference; draw feature neuron activation. Extract SequenceObservedColumnsInferenceSeed/SequenceObservedColumnsInferencePrediction from SequenceObservedColumns. Partially revert inference; fire feature neurons (and activate their connection targets) independently. Patch first_pass; only execute addConceptToConceptColumnsDict if pos in noun_pos_tags. Patch update_observed_columns; set global global_feature_neurons. Update process_features_active(not train):deactivateNeuronsUponPrediction to deactivate based on feature_neurons_inactive_source (i.e. mask by source concept columns only).
