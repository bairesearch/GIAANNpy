DOING//do:
o1 preview prompt 1b1a;

v1b1a:
Please create an abstract biological neural neural network simulation for natural language processing in python. This is the specification:
- Use the pytorch API for hardware acceleration of all arrays (for efficient parallel processing of connectivity). Do not initialise the array with a predetermined dictionary; dynamically populate the arrays as new dictionary words and relations are encountered. Will be discussed below.
- Use vector graphics library such as networkx to always maintain a complete visualisation of the network (including active neurons). Only draw the parts of the network that are not empty.
- The application continuously reads sequences (e.g. sentences) from a large textual corpus from huggingface (such as a Wikipedia dataset). Please use the library nltk.
- It identifies all possible POS (parts of speech) tags of each word in a sequence. Some words may have more than one possible POS tag. Please use the library spacy. 
- Maintain a boolean (testing or production), enable testing mode. To improve the run-time efficiency of the algorithm during testing, set the size of the concept columns dictionary (c) to the maximum number of nouns in the english dictionary, and set the size of the feature arrays (f) to the maximum number of non-nouns in an English dictionary. Obtain these lists of nouns and non-nouns using the nltk wordnet library. We will only assign unique concept columns to nouns in this dictionary (always check POS). We will only assign feature neurons to non-nouns in this dictionary (always check POS).
- Every unique word is assigned a unique column in the network. Maintain a dictionary of observed columns for each word encountered in the dataset while it is being read. Each index in the observed column dictionary corresponds to an index in the concept dictionary of size c (where c is either dynamic during production, or static during testing). Create a class describing observed columns, which contains a list of arrays (pytorch) of f feature neurons, where f is the maximum number of feature neurons per column. Create arrays for neuron strength, neuron permanence, and neuron activation (added to the list). The first feature neuron in the feature array is always the concept neuron of the column (regardless of its name or POS). Note, feature neurons are combined in various combinations to represent different instances of a concept. The observed column class also contains a list of feature connection arrays, defined below.
- Observed column data are brought in/out of RAM from disk on demand. They are brought into RAM from disk for every column word in the sequence, and are saved back to disk after processing every sequence. This includes all array data within the list of feature neuron arrays, and list of feature connection arrays.
- Each column represents a unique concept. Columns (words) can also represent similar concepts (synonyms), but we will ignore semantically similar words for now. We will also ignore the fact words can have multiple meanings for now (i.e. words can represent independent concepts). In this case particular columns will be semantically overloaded (polysemes/homographs).
- If a column detects too many unique proximal features during training it is likely a common word (such as a definite/indefinite article) and will either a) be discarded or b) will not reliably contribute to next word predictions (as a consequence of the way in which connection weights are normalised by the number of feature neurons in a column). After training, the only columns to perform reliable predictions will be those representing concepts (typically substances/nouns). 
- The columns represent general concepts (via the activation of their concept neuron), but they can also represent more specific concepts or instances by incorporating/activating feature neurons. Feature neurons are used to define instances of concepts. There are different classes of feature neurons, including relations such as actions/verbs and conditions/prepositions, qualities/adjectives, or modifiers/adverbs, etc. We are will not be concerned about the specific classes of feature neurons for now; their distinct connectivity should emerge out of the training dynamics of the network.
- The column concepts typically represent substances (nouns), but they can also represent actions or conditions (verbs or prepositions). During testing, column concepts are restricted to substances (nouns). During production, they assigned for every new word encountered in the dataset sequence. 
- For every concept word i in the sequence, identify every feature neuron in that column that occurs q words before or after the concept word in the sequence, including the concept neuron. Note that this is similar to the bag of words algorithm. Increment the corresponding (i.e. contextual) elements (q) of the strength feature neuron array of each sequence word (i) observed concept column object. If a feature neuron has been encountered by a concept column its integer strength value will still be incremented.
- Create column internal and external connections. Connect these feature neurons to each other. Connect these feature neurons to every other feature neuron in every other concept column in the sequence. Connections are made by incrementing an integer strength value in a connection array (initialised as 0). If a connection has already been made this integer strength value will still be incremented.
- Store all connections for each source column in a list of integer feature connection arrays, each of size f * c, where c is the length of the dictionary of columns, and f is the maximum number of feature neurons. The list of feature connection arrays includes a connection strength array (pytorch), connection permanence array, and connection activation array. The direction of a connection created corresponds to the word order of the features in the sequence.
- Set q to 5. The closer the words occur in a sentence, the more likely they are to be connected, but this is not always the case. Consider an example of an exception; "the cat that ate the mouse ran to the park": in this example "ran" and "ate" are directly connected to "cat", "ran" is not directly connected to "mouse" despite being nearest to it.
- You should be aware this this will generate a large set of word associations for each concept word similar to historic NLP algorithms like the word co-occurance matrix.
- Generate all necessary code to run the abstract biological neural network simulation. Visualise the complete graph every time a new sentence is parsed by the application. Let the user exit the simulation at any sentence by pressing Ctrl-D.
- The columns should be visualised along the horizontal x axis (they should each be a tall rectangle). Each concept word neuron should be stored at the bottom of each column. The other feature neurons should be stored along the vertical y axis of each column, above the concept neurons. Draw the connections between all feature neurons and their targets.  
- Please draw separate rectangles for each concept column. The concept column rectangles need to be drawn around the contents (concept and feature neurons) of each column. Ensure the neuron sizes are too large to be encapsulated by the rectangles and all the neurons (including the relation neurons) are drawin within the column rectangles.
- Every feature neuron in a concept column has an integer permanence value (initialised as z1=3). A feature neuron's permanence integer is increased by a function zf every time the feature column neuron is activated by a sequence with that particular feature neuron being activated. A feature neuron's permanence integer is decreased by z2 (set to 1) every time the concept column neuron is activated by a sentence without that particular feature neuron being activated. If a feature neuron's permanence decreases to 0, then the feature neuron is removed from the column and it will no longer be visualised. 
- Likewise, store a permanence value for every feature connection, using the same permanence rules used for feature neurons.
- Every time a feature neuron or connection's permanence is increased (assuming the feature neuron or connection already exists), it exponentially increases its permanence from its current value (via the function zf). Set zf to a squared function of its current integer permanence value. All permanence values are intialised to z1 (3), but do not square them when they are first initialised. Permanence still decreases linearly every time a concept column is activated by the current word but the connection or neuron within it is not referenced by the sequence (-1). The higher the permanence the stronger the weight for next word prediction. We are ignoring next word prediction for now, which involves an iterative topk selection of most activated neurons (similar to hopfield networks).
- Print each sentence in the command line.
- Each sequence word that is contained in the concept dictionary is assigned a unique concept column (and observed column object). Words that are not contained in the concept dictionary are never assigned a concept column (or observed column object).
- Draw the concept neurons blue, and all other feature neurons in cyan.
- Draw all column internal feature connections in yellow, and all column external feature connections in orange. Remember that column internal feature connections and column external feature connections can be identified by their index in the connections array.
- Set an integer activation trace for every neuron and connection in the network. A feature neuron or connection is temporarily excited (set an activation trace value of +1) every time it is referenced by a word in a sequence. This temporary excitation lasts for j=5 sequences before being removed (set an activation trace value of 0), unless it is activated by another sequence in the meantime. If it is activated by another sequence in the meantime, its activation trace is not modified (keep an activation trace value of +1), but overwrite this value in the code (set an activation trace value of +1) in case an implementation change is introduced in the future. The higher the activation trace the stronger the weight for next word prediction. We are not implementing next word prediction yet.
- Perform lemmatisation of each word in sentence before processing it in the neural network. Use a suitable python library, e.g. spacy. Assign concept columns to lemmas not words. Assign feature neurons to words not lemmas.
- Place the main processing loop in a separate function, and place every for loop within the first level (i.e. lowest nesting depth) of the main processing loop in a separate function.
