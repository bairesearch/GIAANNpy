DONE//do:
o1 preview prompt 1b2f;

v1b2f:
Please create an abstract biological neural neural network simulation for natural language processing in python. This is the specification:
- Use the pytorch API for hardware acceleration of all arrays (for efficient parallel processing of connectivity). Do not initialise the array with a predetermined dictionary; dynamically populate the arrays as new dictionary words and relations are encountered. Will be discussed below.
- Use vector graphics library such as networkx to always maintain a complete visualisation of the network (including active neurons). Only draw the parts of the network that are not empty.
- The application continuously reads sequences (i.e. sentences) from a large textual corpus from huggingface (using a streamed Wikipedia dataset). Please use the library nltk.
- It identifies all possible POS (parts of speech) tags of each word in a sequence. Some words may have more than one possible POS tag. Please use the library spacy. 
- Perform lemmatisation of each word in sentence before processing it in the neural network. Use a suitable python library, e.g. spacy. Assign concept columns to lemmas not words. Assign feature neurons to words not lemmas.
- Maintain a boolean (useInference), and disable useInference mode. Maintain a boolean (lowMem), and enable lowMem mode. lowMem can only be used when useInference is disabled. It constrains the memory/computational processing of the network. 
- Maintain a boolean (usePOS), enable usePOS mode. If usePOS is enabled, a validity check of every lemma/word processed is performed to check its POS value. We will only assign unique concept columns for nouns that exist in this dictionary (always check POS). We will only assign feature neurons for non-nouns in this dictionary (always check POS). Nouns and non-nouns are detected using an English dictionary. Obtain lists of nouns and non-nouns using the nltk wordnet library.
- Every valid unique sequence lemma processed in the dataset is assigned a unique column in the network. Add these lemmas as keys in a concept columns dictionary (of dynamic size c). If usePOS is enabled, the validity check is peformed to check if the lemma is a noun before assigning a unique concept column. If usePOS is enabled and lowMem is disabled, set the size of the feature arrays (f) to the maximum number of non-nouns in an English dictionary. Otherwise, set the size of the feature arrays (f) to the current number of concept columns (c).
- Every valid unique sequence lemma processed in the current sequence (not dataset) is assigned an observed column object. Add the lemmas as keys in an observed columns dictionary. Add the observed columns objects as values in this observed columns dictionary. The observed columns dictionary along with its observed column objects are refreshed every time a new sequence is processed, however the dataset concept columns dictionary is maintained. 
- Create a class defining observed columns. The observed column class contains an index to the dataset concept column dictionary. The observed column class contains a list of feature connection arrays. The observed column class also contains a list of feature neuron arrays when useLowMem mode is enabled. The reason connection data must be stored in observed columns is because they are large arrays and every column's connections cannot be stored in RAM simultaneously.
- If lowMem is enabled, the observed columns contain a list of arrays (pytorch) of f feature neurons, where f is the maximum number of feature neurons per column. If lowMem is disabled, create a global list of 2D arrays (pytorch) of c * f feature neurons, where c is the current number of concept columns, and where f is the maximum number of feature neurons per column. Create arrays for neuron strength, neuron permanence, and neuron activation (added to the list). The first feature neuron in the feature array (for each column) is always the concept neuron of the column (regardless of its name or POS). Note, feature neurons are combined in various combinations to represent different instances of a concept (or a specific concept thereof).
- Each column represents a unique concept. Columns (lemmas) can also represent similar concepts (synonyms), but we will ignore semantically similar words for now. We will also ignore the fact words can have multiple meanings for now (i.e. words can represent independent concepts). In this case particular columns will be semantically overloaded (polysemes/homographs).
- If usePOS is disabled, and if a column detects too many unique proximal features during training, it is likely that the column represents a common word (such as a definite/indefinite article) and will either a) be discarded or b) will not reliably contribute to next word predictions (as a consequence of the way in which connection weights are normalised by the number of feature neurons in a column). After training, the only columns to perform reliable predictions will be those representing concepts (typically substances/nouns). 
- The columns represent general concepts (via the activation of their concept neuron), but they can also represent more specific concepts or instances by incorporating/activating feature neurons. Feature neurons are used to define instances of concepts. There are different classes of feature neurons, including relations such as actions/verbs and conditions/prepositions, qualities/adjectives, or modifiers/adverbs, etc. We are will not be concerned about the specific classes of feature neurons for now; their distinct connectivity should emerge out of the training dynamics of the network.
- The column concepts typically represent substances (nouns), but they can also represent actions or conditions (verbs or prepositions). When usePOS is enabled, column concepts are restricted to substances (nouns). When usePOS is disabled, they assigned for every new lemma encountered in the dataset sequence. 
- For every concept word (lemma) i in the sequence, identify every feature neuron in that column that occurs q words before or after the concept word in the sequence, including the concept neuron. Note that this is similar to the bag of words algorithm. Increment the corresponding (i.e. contextual) elements (q) of the strength feature neuron array of each sequence word (i) observed concept column object. If a feature neuron has been encountered by a concept column its integer strength value will still be incremented.
- Create column internal and external connections. Connect these feature neurons to each other. Connect these feature neurons to every other identified feature neuron (observed in the current sequence) in every other concept column in the sequence. Connections are made by incrementing an integer strength value in a connection array (initialised as 0). If a connection has already been made this integer strength value will still be incremented.
- Store all connections for each source column in a list of integer 3D feature connection arrays, each of size f * c * f, where c is the length of the dictionary of columns, and f is the maximum number of feature neurons. The list of feature connection arrays includes a connection strength array (pytorch), connection permanence array, and connection activation array. The direction of a connection created corresponds to the word order of the features in the sequence. The lists of connection arrays are stored in the observed column class objects when in RAM.
- If usePOS is disabled, set q to 5. If usePOS is enabled, set q to the distance to the previous/next noun - 1 (depending on whether the feature selected is before or after the current concept word in the sequence). Always ensure the feature neuron selected is not out of bounds of the sequence. The closer the words occur in a sentence, the more likely they are to be connected, but this is not always the case. Consider an example of an exception; "the cat that ate the mouse ran to the park": in this example "ran" and "ate" are directly connected to "cat", "ran" is not directly connected to "mouse" despite being nearest to it.
- You should be aware this this will generate a large set of word associations for each concept word similar to historic NLP algorithms like the word co-occurance matrix.
- Generate all necessary code to run the abstract biological neural network simulation. Visualise the complete graph every time a new sentence is parsed by the application.
- The columns should be visualised along the horizontal x axis (they should each be a tall rectangle). Each concept word neuron should be stored at the bottom of each column. The other feature neurons should be stored along the vertical y axis of each column, above the concept neurons. Draw the connections between all feature neurons and their targets.  
- Please draw separate rectangles for each concept column. The concept column rectangles need to be drawn around the contents (concept and feature neurons) of each column. Ensure the neuron sizes are too large to be encapsulated by the rectangles and all the neurons (including the relation neurons) are drawin within the column rectangles.
- Print each sentence in the command line.
- Draw the concept neurons blue, and all other feature neurons in cyan.
- Draw all column internal feature connections in yellow, and all column external feature connections in orange. Remember that column internal feature connections and column external feature connections can be identified by their index in the connections array.
- Observed column object data are brought in/out of RAM from disk on demand. They are brought into RAM from disk for every column word in the sequence, and are saved back to disk after processing every sequence. This includes all array data within the list of feature connection arrays (and all array data within the list of feature neurons arrays if lowMem mode is enabled). If lowMem mode is enabled, global feature neurons arrays are brought in/out of RAM from disk before/after processing every sequence.
- The observed column object data and arrays are only brought into RAM after all new unique columns have been detected in the current sequence. These are added to the dataset concept columns dictionary. When the arrays are brought into RAM; if the concept columns dictionary has increased since the processing of the last sequence, the array rows will be expanded to accommodate the new concept columns (with all new rows properly initialised to their default value). If usePOS mode is disabled, the connection array columns will also be expanded to accommodate the new concept columns.
- The concept columns dictionary is also saved to disk after every sentence. When useInference is enabled, the concept columns dictionary is loaded from disk during startup, before processing the first sequence in the dataset.
- When writing the code, add the specification text to comments.
- When writing the code, always add complete code for all modes; do not skip code for disabled modes.
- Place the main processing loop in a separate function (i.e. "for article in dataset:"). Likewise, place every for loop and if statement within the second level of the main processing loop in a separate function (i.e. place every for loop and if statement at the second lowest nesting depth, directly within "for sentence in sentences:" in a separate function).
- Every feature neuron in a concept column has an integer permanence value (initialised as z1=3). A feature neuron's permanence integer is increased by a function zf every time the feature column neuron is activated by a sequence with that particular feature neuron being activated. A feature neuron's permanence integer is decreased by z2 (set to 1) every time the concept column neuron is activated by a sentence without that particular feature neuron being activated. If a feature neuron's permanence decreases to 0, then the feature neuron is removed from the column and it will no longer be visualised. 
- Likewise, store a permanence value for every feature connection, using the same permanence rules used for feature neurons.
- Every time a feature neuron or connection's permanence is increased (assuming the feature neuron or connection already exists), it exponentially increases its permanence from its current value (via the function zf). Set zf to a squared function of its current integer permanence value. All permanence values are intialised to z1 (3), but do not square them when they are first initialised. Permanence still decreases linearly every time a concept column is activated by the current word but the connection or neuron within it is not referenced by the sequence (-1). The higher the permanence the stronger the weight for next word prediction. We are ignoring next word prediction for now, which involves an iterative topk selection of most activated neurons/columns (similar to hopfield networks).
- Set an integer activation trace for every neuron and connection in the network. A feature neuron or connection is temporarily excited (set an activation trace value of +1) every time it is referenced by a word in a sequence. This temporary excitation lasts for j1=5 sequences before being removed (set an activation trace value of 0), unless it is activated by another sequence in the meantime. If it is activated by another sequence in the meantime, its activation trace is not modified (keep an activation trace value of +1), but overwrite this value in the code (set an activation trace value of +1) in case an implementation change is introduced in the future. The higher the activation trace the stronger the weight for next word prediction. We are not implementing next word prediction yet.
FUTURE; - Create a wrapper function called prediction for when useInference mode is enabled, and a wrapper function called training for when useInference mode is disabled. Prediction is similar to training, however the neuron/connection strengths and permanences are never modified (only the activation trace is modified). After processing x seed words in the sequence data (where x=5), the network continues to predict the next most active concept columns in the network. It compares its topk column predictions (set k=1) to the actual words in the sequence data. The observed column object data (i.e. list of feature connection arrays) for the topk predicted columns are brought into RAM after each topk column prediction, and removed from RAM after the next set of topk column predictions. When a concept column has been predicted, the connections of all of its active feature neurons are then activated, to calculate the next topk column predictions. If a concept column has previously been activated in the dataset, its activation trace may positively bias the current topk column selection (depending on the length of time from its last activation). 
CHATONLY/MANUAL; - If usePOS is disabled, set q to 5. If usePOS is enabled, set q to the distance to the previous/next noun - 1 (depending on whether the feature selected is before or after the current concept word in the sequence). Always ensure the feature neuron selected is not out of bounds of the sequence.
CHATONLY; - Expand all the observed column feature connection arrays by 1 dimension (from 2D to 3D) to be each of size f * c * f, where c is the length of the dictionary of columns, and f is the maximum number of feature neurons per column. Update the existing feature connection code; feature neurons are connected to every other identified feature neuron (observed in the current sequence) in every other concept column in the sequence.
MANUAL; - If usePOS is enabled and lowMem is disabled, set the size of the feature arrays (f) to the maximum number of non-nouns in an English dictionary. Otherwise, set the size of the feature arrays (f) to the current number of concept columns (c).
MANUAL; - Create the concept neuron in the feature array of the observed column.
- when usePOS mode is enabled, do not update f and expanding the observed column and global feature arrays after every new feature is detected by an observed column (e.g. process_concept_words:process_feature:expand_feature_arrays). Instead, usePOS mode should update f and expanding the observed column and global feature arrays prior to processing the concept words (e.g. process_concept_words). Create a dedicated function to detect all possible new features in the sequence, and update f and expand the observed column and global feature arrays accordingly. Update the arrays just once for a given sequence, instead of updating them incrementally for every new feature detected in the sequence. When usePOS mode is enabled, all possible new features in the sequence can be detected simply by searching for all new non-nouns in the sequence.
- only draw column feature neurons and connections if their strength is > 0, and their permanence is > 0.

---
You have previously created code based on the following natural language specification;
- Use the pytorch API for hardware acceleration of all arrays (for efficient parallel processing of connectivity). Do not initialise the array with a predetermined dictionary; dynamically populate the arrays as new dictionary words and relations are encountered. Will be discussed below.
- Use vector graphics library such as networkx to always maintain a complete visualisation of the network (including active neurons). Only draw the parts of the network that are not empty.
- The application continuously reads sequences (i.e. sentences) from a large textual corpus from huggingface (using a streamed Wikipedia dataset). Please use the library nltk.
- It identifies all possible POS (parts of speech) tags of each word in a sequence. Some words may have more than one possible POS tag. Please use the library spacy. 
- Perform lemmatisation of each word in sentence before processing it in the neural network. Use a suitable python library, e.g. spacy. Assign concept columns to lemmas not words. Assign feature neurons to words not lemmas.
- Maintain a boolean (useInference), and disable useInference mode. Maintain a boolean (lowMem), and enable lowMem mode. lowMem can only be used when useInference is disabled. It constrains the memory/computational processing of the network. 
- Maintain a boolean (usePOS), enable usePOS mode. If usePOS is enabled, a validity check of every lemma/word processed is performed to check its POS value. We will only assign unique concept columns for nouns that exist in this dictionary (always check POS). We will only assign feature neurons for non-nouns in this dictionary (always check POS). Nouns and non-nouns are detected using an English dictionary. Obtain lists of nouns and non-nouns using the nltk wordnet library.
- Every valid unique sequence lemma processed in the dataset is assigned a unique column in the network. Add these lemmas as keys in a concept columns dictionary (of dynamic size c). If usePOS is enabled, the validity check is peformed to check if the lemma is a noun before assigning a unique concept column. If usePOS is enabled and lowMem is disabled, set the size of the feature arrays (f) to the maximum number of non-nouns in an English dictionary. Otherwise, set the size of the feature arrays (f) to the current number of concept columns (c).
- Every valid unique sequence lemma processed in the current sequence (not dataset) is assigned an observed column object. Add the lemmas as keys in an observed columns dictionary. Add the observed columns objects as values in this observed columns dictionary. The observed columns dictionary along with its observed column objects are refreshed every time a new sequence is processed, however the dataset concept columns dictionary is maintained. 
- Create a class defining observed columns. The observed column class contains an index to the dataset concept column dictionary. The observed column class contains a list of feature connection arrays. The observed column class also contains a list of feature neuron arrays when useLowMem mode is enabled. The reason connection data must be stored in observed columns is because they are large arrays and every column's connections cannot be stored in RAM simultaneously.
- If lowMem is enabled, the observed columns contain a list of arrays (pytorch) of f feature neurons, where f is the maximum number of feature neurons per column. If lowMem is disabled, create a global list of 2D arrays (pytorch) of c * f feature neurons, where c is the current number of concept columns, and where f is the maximum number of feature neurons per column. Create arrays for neuron strength, neuron permanence, and neuron activation (added to the list). The first feature neuron in the feature array (for each column) is always the concept neuron of the column (regardless of its name or POS). Note, feature neurons are combined in various combinations to represent different instances of a concept (or a specific concept thereof).
- Each column represents a unique concept. Columns (lemmas) can also represent similar concepts (synonyms), but we will ignore semantically similar words for now. We will also ignore the fact words can have multiple meanings for now (i.e. words can represent independent concepts). In this case particular columns will be semantically overloaded (polysemes/homographs).
- If usePOS is disabled, and if a column detects too many unique proximal features during training, it is likely that the column represents a common word (such as a definite/indefinite article) and will either a) be discarded or b) will not reliably contribute to next word predictions (as a consequence of the way in which connection weights are normalised by the number of feature neurons in a column). After training, the only columns to perform reliable predictions will be those representing concepts (typically substances/nouns). 
- The columns represent general concepts (via the activation of their concept neuron), but they can also represent more specific concepts or instances by incorporating/activating feature neurons. Feature neurons are used to define instances of concepts. There are different classes of feature neurons, including relations such as actions/verbs and conditions/prepositions, qualities/adjectives, or modifiers/adverbs, etc. We are will not be concerned about the specific classes of feature neurons for now; their distinct connectivity should emerge out of the training dynamics of the network.
- The column concepts typically represent substances (nouns), but they can also represent actions or conditions (verbs or prepositions). When usePOS is enabled, column concepts are restricted to substances (nouns). When usePOS is disabled, they assigned for every new lemma encountered in the dataset sequence. 
- For every concept word (lemma) i in the sequence, identify every feature neuron in that column that occurs q words before or after the concept word in the sequence, including the concept neuron. Note that this is similar to the bag of words algorithm. Increment the corresponding (i.e. contextual) elements (q) of the strength feature neuron array of each sequence word (i) observed concept column object. If a feature neuron has been encountered by a concept column its integer strength value will still be incremented.
- Create column internal and external connections. Connect these feature neurons to each other. Connect these feature neurons to every other identified feature neuron (observed in the current sequence) in every other concept column in the sequence. Connections are made by incrementing an integer strength value in a connection array (initialised as 0). If a connection has already been made this integer strength value will still be incremented.
- Store all connections for each source column in a list of integer 3D feature connection arrays, each of size f * c * f, where c is the length of the dictionary of columns, and f is the maximum number of feature neurons. The list of feature connection arrays includes a connection strength array (pytorch), connection permanence array, and connection activation array. The direction of a connection created corresponds to the word order of the features in the sequence. The lists of connection arrays are stored in the observed column class objects when in RAM.
- If usePOS is disabled, set q to 5. If usePOS is enabled, set q to the distance to the previous/next noun - 1 (depending on whether the feature selected is before or after the current concept word in the sequence). Always ensure the feature neuron selected is not out of bounds of the sequence. The closer the words occur in a sentence, the more likely they are to be connected, but this is not always the case. Consider an example of an exception; "the cat that ate the mouse ran to the park": in this example "ran" and "ate" are directly connected to "cat", "ran" is not directly connected to "mouse" despite being nearest to it.
- You should be aware this this will generate a large set of word associations for each concept word similar to historic NLP algorithms like the word co-occurance matrix.
- Generate all necessary code to run the abstract biological neural network simulation. Visualise the complete graph every time a new sentence is parsed by the application.
- The columns should be visualised along the horizontal x axis (they should each be a tall rectangle). Each concept word neuron should be stored at the bottom of each column. The other feature neurons should be stored along the vertical y axis of each column, above the concept neurons. Draw the connections between all feature neurons and their targets.  
- Please draw separate rectangles for each concept column. The concept column rectangles need to be drawn around the contents (concept and feature neurons) of each column. Ensure the neuron sizes are too large to be encapsulated by the rectangles and all the neurons (including the relation neurons) are drawin within the column rectangles.
- Print each sentence in the command line.
- Draw the concept neurons blue, and all other feature neurons in cyan.
- Draw all column internal feature connections in yellow, and all column external feature connections in orange. Remember that column internal feature connections and column external feature connections can be identified by their index in the connections array.
- Observed column object data are brought in/out of RAM from disk on demand. They are brought into RAM from disk for every column word in the sequence, and are saved back to disk after processing every sequence. This includes all array data within the list of feature connection arrays (and all array data within the list of feature neurons arrays if lowMem mode is enabled). If lowMem mode is enabled, global feature neurons arrays are brought in/out of RAM from disk before/after processing every sequence.
- The observed column object data and arrays are only brought into RAM after all new unique columns have been detected in the current sequence. These are added to the dataset concept columns dictionary. When the arrays are brought into RAM; if the concept columns dictionary has increased since the processing of the last sequence, the array rows will be expanded to accommodate the new concept columns (with all new rows properly initialised to their default value). If usePOS mode is disabled, the connection array columns will also be expanded to accommodate the new concept columns.
- The concept columns dictionary is also saved to disk after every sentence. When useInference is enabled, the concept columns dictionary is loaded from disk during startup, before processing the first sequence in the dataset.
- When writing the code, add the specification text to comments.
- When writing the code, always add complete code for all modes; do not skip code for disabled modes.
- Place the main processing loop in a separate function (i.e. "for article in dataset:"). Likewise, place every for loop and if statement within the second level of the main processing loop in a separate function (i.e. place every for loop and if statement at the second lowest nesting depth, directly within "for sentence in sentences:" in a separate function).
- Every feature neuron in a concept column has an integer permanence value (initialised as z1=3). A feature neuron's permanence integer is increased by a function zf every time the feature column neuron is activated by a sequence with that particular feature neuron being activated. A feature neuron's permanence integer is decreased by z2 (set to 1) every time the concept column neuron is activated by a sentence without that particular feature neuron being activated. If a feature neuron's permanence decreases to 0, then the feature neuron is removed from the column and it will no longer be visualised. 
- Likewise, store a permanence value for every feature connection, using the same permanence rules used for feature neurons.
- Every time a feature neuron or connection's permanence is increased (assuming the feature neuron or connection already exists), it exponentially increases its permanence from its current value (via the function zf). Set zf to a squared function of its current integer permanence value. All permanence values are intialised to z1 (3), but do not square them when they are first initialised. Permanence still decreases linearly every time a concept column is activated by the current word but the connection or neuron within it is not referenced by the sequence (-1). The higher the permanence the stronger the weight for next word prediction. We are ignoring next word prediction for now, which involves an iterative topk selection of most activated neurons/columns (similar to hopfield networks).
- Set an integer activation trace for every neuron and connection in the network. A feature neuron or connection is temporarily excited (set an activation trace value of +1) every time it is referenced by a word in a sequence. This temporary excitation lasts for j1=5 sequences before being removed (set an activation trace value of 0), unless it is activated by another sequence in the meantime. If it is activated by another sequence in the meantime, its activation trace is not modified (keep an activation trace value of +1), but overwrite this value in the code (set an activation trace value of +1) in case an implementation change is introduced in the future. The higher the activation trace the stronger the weight for next word prediction. We are not implementing next word prediction yet.
- when usePOS mode is enabled, do not update f and expanding the observed column and global feature arrays after every new feature is detected by an observed column (e.g. process_concept_words:process_feature:expand_feature_arrays). Instead, usePOS mode should update f and expanding the observed column and global feature arrays prior to processing the concept words (e.g. process_concept_words). Create a dedicated function to detect all possible new features in the sequence, and update f and expand the observed column and global feature arrays accordingly. Update the arrays just once for a given sequence, instead of updating them incrementally for every new feature detected in the sequence. When usePOS mode is enabled, all possible new features in the sequence can be detected simply by searching for all new non-nouns in the sequence.

Please modify the code by implementing this change:
- only draw column feature neurons and connections if their strength is > 0, and their permanence is > 0.

Existing Code:
# Import necessary libraries
import torch
import networkx as nx
import matplotlib.pyplot as plt
import nltk
import spacy
from datasets import load_dataset
import os
import pickle

# Download required NLTK data
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('omw-1.4')

from nltk.corpus import wordnet as wn
from nltk.tokenize import sent_tokenize

# Set boolean variables as per specification
useInference = False  # Disable useInference mode
lowMem = True         # Enable lowMem mode (can only be used when useInference is disabled)
usePOS = True         # Enable usePOS mode

# Paths for saving data
concept_columns_dict_file = 'concept_columns_dict.pkl'
observed_columns_dir = 'observed_columns'
os.makedirs(observed_columns_dir, exist_ok=True)

if not lowMem:
    feature_neurons_strength_file = 'global_feature_neurons_strength.pt'
    feature_neurons_permanence_file = 'global_feature_neurons_permanence.pt'
    feature_neurons_activation_file = 'global_feature_neurons_activation.pt'

# Obtain lists of nouns and non-nouns using the NLTK wordnet library
nouns = set()
for synset in wn.all_synsets('n'):
    for lemma in synset.lemma_names():
        nouns.add(lemma.lower())

all_words = set()
for synset in wn.all_synsets():
    for lemma in synset.lemma_names():
        all_words.add(lemma.lower())

non_nouns = all_words - nouns
max_num_non_nouns = len(non_nouns)

# Set the size of the feature arrays (f)
if usePOS and not lowMem:
    f = max_num_non_nouns  # Maximum number of non-nouns in an English dictionary
else:
    f = 0  # Will be updated dynamically based on c

# Initialize the concept columns dictionary
if useInference and os.path.exists(concept_columns_dict_file):
    with open(concept_columns_dict_file, 'rb') as f_in:
        concept_columns_dict = pickle.load(f_in)
    c = len(concept_columns_dict)
    concept_columns_list = list(concept_columns_dict.keys())
else:
    concept_columns_dict = {}  # key: lemma, value: index
    concept_columns_list = []  # list of concept column names (lemmas)
    c = 0  # current number of concept columns

# Initialize global feature neuron arrays if lowMem is disabled
if not lowMem:
    if os.path.exists(feature_neurons_strength_file):
        global_feature_neurons_strength = torch.load(feature_neurons_strength_file)
        global_feature_neurons_permanence = torch.load(feature_neurons_permanence_file)
        global_feature_neurons_activation = torch.load(feature_neurons_activation_file)
    else:
        global_feature_neurons_strength = torch.zeros(c, f)
        global_feature_neurons_permanence = torch.full((c, f), 3)  # Initialize permanence to z1=3
        global_feature_neurons_activation = torch.zeros(c, f, dtype=torch.int32)  # Activation trace

# Initialize spaCy model
nlp = spacy.load('en_core_web_sm')

# Define POS tag sets for nouns and non-nouns
noun_pos_tags = {'NOUN', 'PROPN'}
non_noun_pos_tags = {'ADJ', 'ADV', 'VERB', 'ADP', 'AUX', 'CCONJ', 'DET', 'INTJ',
                     'NUM', 'PART', 'PRON', 'SCONJ', 'SYM', 'X'}

# Define constants for permanence and activation trace
z1 = 3  # Initial permanence value
z2 = 1  # Decrement value when not activated
j1 = 5   # Activation trace duration

# Define the ObservedColumn class
class ObservedColumn:
    """
    Create a class defining observed columns. The observed column class contains an index to the
    dataset concept column dictionary. The observed column class contains a list of feature
    connection arrays. The observed column class also contains a list of feature neuron arrays
    when lowMem mode is enabled.
    """
    def __init__(self, concept_index, lemma):
        self.concept_index = concept_index  # Index to the concept columns dictionary

        if lowMem:
            # If lowMem is enabled, the observed columns contain a list of arrays (pytorch)
            # of f feature neurons, where f is the maximum number of feature neurons per column.
            self.feature_neurons_strength = torch.zeros(f)
            self.feature_neurons_permanence = torch.full((f,), z1, dtype=torch.int32)  # Initialize permanence to z1=3
            self.feature_neurons_activation = torch.zeros(f, dtype=torch.int32)  # Activation trace counters

        # Map from feature words to indices in feature neurons
        self.feature_word_to_index = {}  # Maps feature words to indices
        self.feature_index_to_word = {}  # Maps indices to feature words
        self.next_feature_index = 1  # Start from 1 since index 0 is reserved for concept neuron
        self.feature_word_to_index[lemma] = 0
        self.feature_index_to_word[0] = lemma

        # Store all connections for each source column in a list of integer feature connection arrays,
        # each of size f * c * f, where c is the length of the dictionary of columns, and f is the maximum
        # number of feature neurons.
        self.connection_strength = torch.zeros(f, c, f, dtype=torch.int32)
        self.connection_permanence = torch.full((f, c, f), z1, dtype=torch.int32)  # Initialize permanence to z1=3
        self.connection_activation = torch.zeros(f, c, f, dtype=torch.int32)  # Activation trace counters

    def resize_connection_arrays(self, new_c):
        if new_c > self.connection_strength.shape[1]:
            extra_cols = new_c - self.connection_strength.shape[1]
            # Expand along dimension 1 (columns)
            self.connection_strength = torch.cat([self.connection_strength, torch.zeros(self.connection_strength.shape[0], extra_cols, self.connection_strength.shape[2], dtype=torch.int32)], dim=1)
            self.connection_permanence = torch.cat([self.connection_permanence, torch.full((self.connection_permanence.shape[0], extra_cols, self.connection_permanence.shape[2]), z1, dtype=torch.int32)], dim=1)
            self.connection_activation = torch.cat([self.connection_activation, torch.zeros(self.connection_activation.shape[0], extra_cols, self.connection_activation.shape[2], dtype=torch.int32)], dim=1)

    def expand_feature_arrays(self, new_f):
        if new_f > self.connection_strength.shape[0]:
            extra_rows = new_f - self.connection_strength.shape[0]
            # Expand along dimension 0 (rows) and dimension 2
            self.connection_strength = torch.cat([self.connection_strength, torch.zeros(extra_rows, self.connection_strength.shape[1], self.connection_strength.shape[2], dtype=torch.int32)], dim=0)
            self.connection_permanence = torch.cat([self.connection_permanence, torch.full((extra_rows, self.connection_permanence.shape[1], self.connection_permanence.shape[2]), z1, dtype=torch.int32)], dim=0)
            self.connection_activation = torch.cat([self.connection_activation, torch.zeros(extra_rows, self.connection_activation.shape[1], self.connection_activation.shape[2], dtype=torch.int32)], dim=0)

            # Also expand along dimension 2
            extra_slices = new_f - self.connection_strength.shape[2]
            self.connection_strength = torch.cat([self.connection_strength, torch.zeros(self.connection_strength.shape[0], self.connection_strength.shape[1], extra_slices, dtype=torch.int32)], dim=2)
            self.connection_permanence = torch.cat([self.connection_permanence, torch.full((self.connection_permanence.shape[0], self.connection_permanence.shape[1], extra_slices), z1, dtype=torch.int32)], dim=2)
            self.connection_activation = torch.cat([self.connection_activation, torch.zeros(self.connection_activation.shape[0], self.connection_activation.shape[1], extra_slices, dtype=torch.int32)], dim=2)

            if lowMem:
                self.feature_neurons_strength = torch.cat([self.feature_neurons_strength, torch.zeros(extra_rows)], dim=0)
                self.feature_neurons_permanence = torch.cat([self.feature_neurons_permanence, torch.full((extra_rows,), z1, dtype=torch.int32)], dim=0)
                self.feature_neurons_activation = torch.cat([self.feature_neurons_activation, torch.zeros(extra_rows, dtype=torch.int32)], dim=0)

    def save_to_disk(self):
        """
        Save the observed column data to disk.
        """
        data = {
            'concept_index': self.concept_index,
            'feature_word_to_index': self.feature_word_to_index,
            'feature_index_to_word': self.feature_index_to_word,
            'next_feature_index': self.next_feature_index
        }
        # Save the data dictionary using pickle
        with open(os.path.join(observed_columns_dir, f"{self.concept_index}_data.pkl"), 'wb') as f:
            pickle.dump(data, f)
        # Save the tensors using torch.save
        torch.save(self.connection_strength, os.path.join(observed_columns_dir, f"{self.concept_index}_connection_strength.pt"))
        torch.save(self.connection_permanence, os.path.join(observed_columns_dir, f"{self.concept_index}_connection_permanence.pt"))
        torch.save(self.connection_activation, os.path.join(observed_columns_dir, f"{self.concept_index}_connection_activation.pt"))
        if lowMem:
            torch.save(self.feature_neurons_strength, os.path.join(observed_columns_dir, f"{self.concept_index}_feature_neurons_strength.pt"))
            torch.save(self.feature_neurons_permanence, os.path.join(observed_columns_dir, f"{self.concept_index}_feature_neurons_permanence.pt"))
            torch.save(self.feature_neurons_activation, os.path.join(observed_columns_dir, f"{self.concept_index}_feature_neurons_activation.pt"))

    @classmethod
    def load_from_disk(cls, concept_index, lemma):
        """
        Load the observed column data from disk.
        """
        # Load the data dictionary
        with open(os.path.join(observed_columns_dir, f"{concept_index}_data.pkl"), 'rb') as f:
            data = pickle.load(f)
        instance = cls(concept_index, lemma)
        instance.feature_word_to_index = data['feature_word_to_index']
        instance.feature_index_to_word = data['feature_index_to_word']
        instance.next_feature_index = data['next_feature_index']
        # Load the tensors
        instance.connection_strength = torch.load(os.path.join(observed_columns_dir, f"{concept_index}_connection_strength.pt"))
        instance.connection_permanence = torch.load(os.path.join(observed_columns_dir, f"{concept_index}_connection_permanence.pt"))
        instance.connection_activation = torch.load(os.path.join(observed_columns_dir, f"{concept_index}_connection_activation.pt"))
        if lowMem:
            instance.feature_neurons_strength = torch.load(os.path.join(observed_columns_dir, f"{concept_index}_feature_neurons_strength.pt"))
            instance.feature_neurons_permanence = torch.load(os.path.join(observed_columns_dir, f"{concept_index}_feature_neurons_permanence.pt"))
            instance.feature_neurons_activation = torch.load(os.path.join(observed_columns_dir, f"{concept_index}_feature_neurons_activation.pt"))
        return instance

# Initialize NetworkX graph for visualization
G = nx.Graph()

# For the purpose of the example, process a limited number of sentences
sentence_count = 0
max_sentences = 5  # Adjust as needed

def process_dataset(dataset):
    global sentence_count
    for article in dataset:
        process_article(article)
        if sentence_count >= max_sentences:
            break

def process_article(article):
    global sentence_count
    sentences = sent_tokenize(article['text'])
    for sentence in sentences:
        process_sentence(sentence)
        if sentence_count >= max_sentences:
            break

def process_sentence(sentence):
    global sentence_count, c, f, concept_columns_dict, concept_columns_list
    print(f"Processing sentence: {sentence}")

    # Refresh the observed columns dictionary for each new sequence
    observed_columns_dict = {}  # key: lemma, value: ObservedColumn

    # Process the sentence with spaCy
    doc = nlp(sentence)

    # First pass: Extract words, lemmas, POS tags, and update concept_columns_dict and c
    words, lemmas, pos_tags = first_pass(doc)

    # Second pass: Create observed_columns_dict
    observed_columns_dict = second_pass(lemmas, pos_tags)

    # When usePOS is enabled, detect all possible new features in the sequence
    if usePOS:
        detect_new_features(words, lemmas, pos_tags, observed_columns_dict)

    # Process each observed column to ensure connection arrays are resized if needed
    for observed_column in observed_columns_dict.values():
        observed_column.resize_connection_arrays(c)
        # Also need to expand feature arrays if f has increased
        observed_column.expand_feature_arrays(f)

    # Process each concept word in the sequence
    process_concept_words(doc, lemmas, pos_tags, observed_columns_dict)

    # Update permanence and activation traces for feature neurons and connections
    update_permanence_and_activation(observed_columns_dict)

    # Visualize the complete graph every time a new sentence is parsed by the application.
    visualize_graph(observed_columns_dict)

    # Save observed columns to disk
    save_data(observed_columns_dict)

    # Break if we've reached the maximum number of sentences
    global sentence_count
    sentence_count += 1

def first_pass(doc):
    global c, f, concept_columns_dict, concept_columns_list
    if not lowMem:
        global global_feature_neurons_strength, global_feature_neurons_permanence, global_feature_neurons_activation
    words = []
    lemmas = []
    pos_tags = []
    new_concepts_added = False

    for token in doc:
        word = token.text
        lemma = token.lemma_.lower()
        pos = token.pos_  # Part-of-speech tag

        words.append(word)
        lemmas.append(lemma)
        pos_tags.append(pos)

        if usePOS:
            if pos in noun_pos_tags:
                # Only assign unique concept columns for nouns
                if lemma not in concept_columns_dict:
                    # Add to concept columns dictionary
                    concept_columns_dict[lemma] = c
                    concept_columns_list.append(lemma)
                    c += 1
                    new_concepts_added = True
        else:
            # When usePOS is disabled, assign concept columns for every new lemma encountered
            if lemma not in concept_columns_dict:
                concept_columns_dict[lemma] = c
                concept_columns_list.append(lemma)
                c += 1
                new_concepts_added = True

    # If new concept columns have been added, expand arrays as needed
    if new_concepts_added:
        if not lowMem:
            # Expand global feature neuron arrays
            if global_feature_neurons_strength.shape[0] < c:
                extra_rows = c - global_feature_neurons_strength.shape[0]
                global_feature_neurons_strength = torch.cat([global_feature_neurons_strength, torch.zeros(extra_rows, f)], dim=0)
                global_feature_neurons_permanence = torch.cat([global_feature_neurons_permanence, torch.full((extra_rows, f), z1, dtype=torch.int32)], dim=0)
                global_feature_neurons_activation = torch.cat([global_feature_neurons_activation, torch.zeros(extra_rows, f, dtype=torch.int32)], dim=0)

    return words, lemmas, pos_tags

def second_pass(lemmas, pos_tags):
    observed_columns_dict = {}
    for i, lemma in enumerate(lemmas):
        pos = pos_tags[i]
        if usePOS:
            if pos in noun_pos_tags:
                concept_index = concept_columns_dict[lemma]
                # Load observed column from disk or create new one
                observed_column = load_or_create_observed_column(concept_index, lemma)
                observed_columns_dict[lemma] = observed_column
        else:
            concept_index = concept_columns_dict[lemma]
            # Load observed column from disk or create new one
            observed_column = load_or_create_observed_column(concept_index, lemma)
            observed_columns_dict[lemma] = observed_column
    return observed_columns_dict

def load_or_create_observed_column(concept_index, lemma=None):
    observed_column_file = os.path.join(observed_columns_dir, f"{concept_index}_data.pkl")
    if os.path.exists(observed_column_file):
        observed_column = ObservedColumn.load_from_disk(concept_index, lemma)
        # Resize connection arrays if c has increased
        observed_column.resize_connection_arrays(c)
        # Also expand feature arrays if f has increased
        observed_column.expand_feature_arrays(f)
    else:
        observed_column = ObservedColumn(concept_index, lemma)
        # Initialize connection arrays with correct size
        observed_column.resize_connection_arrays(c)
        observed_column.expand_feature_arrays(f)
    return observed_column

def detect_new_features(words, lemmas, pos_tags, observed_columns_dict):
    """
    When usePOS mode is enabled, detect all possible new features in the sequence
    by searching for all new non-nouns in the sequence.
    """
    global f, lowMem, global_feature_neurons_strength, global_feature_neurons_permanence, global_feature_neurons_activation

    max_feature_index = 0

    for i, lemma_i in enumerate(lemmas):
        if lemma_i in observed_columns_dict:
            observed_column = observed_columns_dict[lemma_i]
            for j, (word_j, pos_j) in enumerate(zip(words, pos_tags)):
                process_feature_detection(observed_column, i, j, word_j, pos_tags)

    # After processing all features, update f
    f = max([col.next_feature_index for col in observed_columns_dict.values()])
    # Now, expand arrays accordingly
    if not lowMem:
        if f > global_feature_neurons_strength.shape[1]:
            extra_cols = f - global_feature_neurons_strength.shape[1]
            global_feature_neurons_strength = torch.cat([global_feature_neurons_strength, torch.zeros(global_feature_neurons_strength.shape[0], extra_cols)], dim=1)
            global_feature_neurons_permanence = torch.cat([global_feature_neurons_permanence, torch.full((global_feature_neurons_permanence.shape[0], extra_cols), z1, dtype=torch.int32)], dim=1)
            global_feature_neurons_activation = torch.cat([global_feature_neurons_activation, torch.zeros(global_feature_neurons_activation.shape[0], extra_cols, dtype=torch.int32)], dim=1)

    # Expand feature arrays in observed columns
    for observed_column in observed_columns_dict.values():
        observed_column.expand_feature_arrays(f)

def process_feature_detection(observed_column, i, j, word_j, pos_tags):
    """
    Helper function to detect new features prior to processing concept words.
    """
    pos_j = pos_tags[j]
    feature_word = word_j.lower()
    
    if pos_j in noun_pos_tags:
        return  # Skip nouns as features

    if feature_word not in observed_column.feature_word_to_index:
        feature_index = observed_column.next_feature_index
        observed_column.feature_word_to_index[feature_word] = feature_index
        observed_column.feature_index_to_word[feature_index] = feature_word
        observed_column.next_feature_index += 1

def process_concept_words(doc, lemmas, pos_tags, observed_columns_dict):
    """
    For every concept word (lemma) i in the sequence, identify every feature neuron in that column
    that occurs q words before or after the concept word in the sequence, including the concept neuron.
    If usePOS is disabled, set q to 5. If usePOS is enabled, set q to the distance to the previous/next noun
    (depending on whether the feature selected is before or after the current concept word in the sequence).
    Always ensure the feature neuron selected is not out of bounds of the sequence.
    """
    global c, f, lowMem, global_feature_neurons_strength, global_feature_neurons_permanence, global_feature_neurons_activation
    if usePOS:
        # Precompute noun indices
        noun_indices = [index for index, pos in enumerate(pos_tags) if pos in noun_pos_tags]
    else:
        q = 5

    for i, token in enumerate(doc):
        lemma_i = lemmas[i]
        pos_i = pos_tags[i]

        if lemma_i in observed_columns_dict:
            observed_column = observed_columns_dict[lemma_i]
            concept_index_i = observed_column.concept_index

            # Set to track feature neurons activated in this sequence for this concept
            activated_feature_indices = set()

            if usePOS:
                # Compute distances to previous and next nouns
                # Get index of previous noun before i
                prev_noun_index = None
                for idx in range(i - 1, -1, -1):
                    if pos_tags[idx] in noun_pos_tags:
                        prev_noun_index = idx
                        break
                dist_to_prev_noun = i - prev_noun_index - 1 if prev_noun_index is not None else i

                # Get index of next noun after i
                next_noun_index = None
                for idx in range(i + 1, len(doc)):
                    if pos_tags[idx] in noun_pos_tags:
                        next_noun_index = idx
                        break
                dist_to_next_noun = next_noun_index - i - 1 if next_noun_index is not None else len(doc) - i - 1
            else:
                q = 5

            # Process positions before i
            if usePOS:
                start = max(0, i - dist_to_prev_noun)
            else:
                start = max(0, i - q)

            for j in range(start, i):
                process_feature(observed_column, i, j, doc, lemmas, pos_tags, activated_feature_indices, observed_columns_dict)

            # Process positions after i
            if usePOS:
                end = min(len(doc), i + dist_to_next_noun + 1)
            else:
                end = min(len(doc), i + q + 1)

            for j in range(i + 1, end):
                process_feature(observed_column, i, j, doc, lemmas, pos_tags, activated_feature_indices, observed_columns_dict)

            # Decrease permanence for feature neurons not activated
            all_feature_indices = set(observed_column.feature_word_to_index.values())
            inactive_feature_indices = all_feature_indices - activated_feature_indices
            for feature_index in inactive_feature_indices:
                # Decrease permanence linearly
                if lowMem:
                    observed_column.feature_neurons_permanence[feature_index] -= z2
                    # Remove feature neuron if permanence <= 0
                    if observed_column.feature_neurons_permanence[feature_index] <= 0:
                        # Remove feature neuron from the column
                        del_word = observed_column.feature_index_to_word[feature_index]
                        del observed_column.feature_word_to_index[del_word]
                        del observed_column.feature_index_to_word[feature_index]
                        # Set permanence and activation to zero
                        observed_column.feature_neurons_permanence[feature_index] = 0
                        observed_column.feature_neurons_activation[feature_index] = 0
                else:
                    global_feature_neurons_permanence[concept_index_i, feature_index] -= z2
                    # Remove feature neuron if permanence <= 0
                    if global_feature_neurons_permanence[concept_index_i, feature_index] <= 0:
                        # Set permanence and activation to zero
                        global_feature_neurons_permanence[concept_index_i, feature_index] = 0
                        global_feature_neurons_activation[concept_index_i, feature_index] = 0

            # Decrease permanence for connections not activated
            for feature_index in inactive_feature_indices:
                for other_concept_index in range(c):
                    if other_concept_index != concept_index_i:
                        other_observed_column = load_or_create_observed_column(other_concept_index)
                        all_other_feature_indices = set(other_observed_column.feature_word_to_index.values())
                        for other_feature_index in all_other_feature_indices:
                            if observed_column.connection_permanence[feature_index, other_concept_index, other_feature_index] > 0:
                                observed_column.connection_permanence[feature_index, other_concept_index, other_feature_index] -= z2
                                # Remove connection if permanence <= 0
                                if observed_column.connection_permanence[feature_index, other_concept_index, other_feature_index] <= 0:
                                    observed_column.connection_permanence[feature_index, other_concept_index, other_feature_index] = 0
                                    observed_column.connection_activation[feature_index, other_concept_index, other_feature_index] = 0

def process_feature(observed_column, i, j, doc, lemmas, pos_tags, activated_feature_indices, observed_columns_dict):
    """
    Helper function to process a feature at position j for the concept at position i.
    """
    global c, f, lowMem, global_feature_neurons_strength, global_feature_neurons_permanence, global_feature_neurons_activation
    lemma_i = lemmas[i]
    lemma_j = lemmas[j]
    pos_j = pos_tags[j]
    token_j = doc[j]
    word_j = token_j.text

    # Assign feature neurons to words not lemmas
    feature_word = word_j.lower()

    # Skip nouns as features when usePOS is enabled
    if usePOS and pos_j in noun_pos_tags:
        return

    # Get feature neuron index for feature_word in this column
    if feature_word not in observed_column.feature_word_to_index:
        if usePOS:
            print("Should not happen as features are pre-detected")
            return
        else:
            feature_index = observed_column.next_feature_index
            observed_column.feature_word_to_index[feature_word] = feature_index
            observed_column.feature_index_to_word[feature_index] = feature_word
            observed_column.next_feature_index += 1
            # Expand feature arrays if needed
            if feature_index >= f:
                f = feature_index + 1
                if not lowMem:
                    # Expand global feature neuron arrays
                    extra_cols = f - global_feature_neurons_strength.shape[1]
                    global_feature_neurons_strength = torch.cat([global_feature_neurons_strength, torch.zeros(global_feature_neurons_strength.shape[0], extra_cols)], dim=1)
                    global_feature_neurons_permanence = torch.cat([global_feature_neurons_permanence, torch.full((global_feature_neurons_permanence.shape[0], extra_cols), z1, dtype=torch.int32)], dim=1)
                    global_feature_neurons_activation = torch.cat([global_feature_neurons_activation, torch.zeros(global_feature_neurons_activation.shape[0], extra_cols, dtype=torch.int32)], dim=1)
                # Expand connection and feature arrays for all observed columns
                for obs_col in observed_columns_dict.values():
                    obs_col.expand_feature_arrays(f)
    else:
        feature_index = observed_column.feature_word_to_index[feature_word]

    # Add feature index to activated set
    activated_feature_indices.add(feature_index)

    # Increment the strength of the feature neuron
    if lowMem:
        observed_column.feature_neurons_strength[feature_index] += 1
        # Increase permanence exponentially
        observed_column.feature_neurons_permanence[feature_index] = observed_column.feature_neurons_permanence[feature_index] ** 2
        # Set activation trace to j1 sequences
        observed_column.feature_neurons_activation[feature_index] = j1  # Overwrite with j1
    else:
        concept_index_i = observed_column.concept_index
        global_feature_neurons_strength[concept_index_i, feature_index] += 1
        # Increase permanence exponentially
        global_feature_neurons_permanence[concept_index_i, feature_index] = global_feature_neurons_permanence[concept_index_i, feature_index] ** 2
        # Set activation trace to j1 sequences
        global_feature_neurons_activation[concept_index_i, feature_index] = j1

    # Create connections
    # Connect these feature neurons to every other identified feature neuron (observed in the current sequence) in every other concept column in the sequence
    for other_lemma, other_observed_column in observed_columns_dict.items():
        other_concept_index = other_observed_column.concept_index
        if other_concept_index != observed_column.concept_index:
            for other_feature_word, other_feature_index in other_observed_column.feature_word_to_index.items():
                # Update the connection arrays
                observed_column.connection_strength[feature_index, other_concept_index, other_feature_index] += 1
                # Increase permanence exponentially
                observed_column.connection_permanence[feature_index, other_concept_index, other_feature_index] = observed_column.connection_permanence[feature_index, other_concept_index, other_feature_index] ** 2
                # Set activation trace to j1 sequences
                observed_column.connection_activation[feature_index, other_concept_index, other_feature_index] = j1

def update_permanence_and_activation(observed_columns_dict):
    # For each observed column, update activation traces
    for observed_column in observed_columns_dict.values():
        # Feature neurons
        if lowMem:
            active_indices = observed_column.feature_neurons_activation.nonzero(as_tuple=True)[0]
            for idx in active_indices:
                idx = idx.item()  # Convert tensor to integer
                if observed_column.feature_neurons_activation[idx] > 0:
                    observed_column.feature_neurons_activation[idx] -= 1
                    if observed_column.feature_neurons_activation[idx] == 0:
                        # Activation trace expired
                        pass  # Do nothing for now

        # Connections
        active_indices = observed_column.connection_activation.nonzero(as_tuple=False)
        for idx in active_indices:
            i = idx[0].item()
            j = idx[1].item()
            k = idx[2].item()
            if observed_column.connection_activation[i, j, k] > 0:
                observed_column.connection_activation[i, j, k] -= 1
                if observed_column.connection_activation[i, j, k] == 0:
                    # Activation trace expired
                    pass  # Do nothing for now

def visualize_graph(observed_columns_dict):
    G.clear()

    # Draw concept columns
    pos_dict = {}
    x_offset = 0
    for lemma, observed_column in observed_columns_dict.items():
        concept_index = observed_column.concept_index

        # Draw feature neurons
        y_offset = 1
        for feature_index, feature_word in observed_column.feature_index_to_word.items():
            if feature_index == 0:
                neuron_color = 'blue'
            else:
                neuron_color = 'cyan'
            if lowMem:
                # Only visualize feature neurons with permanence > 0
                if observed_column.feature_neurons_permanence[feature_index] > 0:
                    feature_node = f"{lemma}_{feature_word}_{feature_index}"
                    G.add_node(feature_node, pos=(x_offset, y_offset), color=neuron_color, label=feature_word)
                    y_offset += 1
            else:
                feature_node = f"{lemma}_{feature_word}_{feature_index}"
                G.add_node(feature_node, pos=(x_offset, y_offset), color=neuron_color, label=feature_word)
                y_offset += 1

        # Draw rectangle around the column
        plt.gca().add_patch(plt.Rectangle((x_offset - 0.5, -0.5), 1, max(y_offset, 1) + 0.5, fill=False, edgecolor='black'))

        x_offset += 2  # Adjust x_offset for the next column

    # Draw connections
    for lemma, observed_column in observed_columns_dict.items():
        concept_index = observed_column.concept_index

        # Internal connections (yellow)
        for feature_index, feature_word in observed_column.feature_index_to_word.items():
            source_node = f"{lemma}_{feature_word}_{feature_index}"
            if G.has_node(source_node):
                for other_feature_index, other_feature_word in observed_column.feature_index_to_word.items():
                    target_node = f"{lemma}_{other_feature_word}_{other_feature_index}"
                    if G.has_node(target_node):
                        if feature_index != other_feature_index:
                            G.add_edge(source_node, target_node, color='yellow')

        # External connections (orange)
        for feature_index, feature_word in observed_column.feature_index_to_word.items():
            source_node = f"{lemma}_{feature_word}_{feature_index}"
            if G.has_node(source_node):
                for other_concept_index in range(c):
                    if other_concept_index != concept_index:
                        other_lemma = concept_columns_list[other_concept_index]
                        other_observed_column = observed_columns_dict.get(other_lemma)
                        if other_observed_column is not None:
                            for other_feature_index, other_feature_word in other_observed_column.feature_index_to_word.items():
                                target_node = f"{other_lemma}_{other_feature_word}_{other_feature_index}"
                                if G.has_node(target_node):
                                    # Only visualize connections with permanence > 0
                                    if observed_column.connection_permanence[feature_index, other_concept_index, other_feature_index] > 0:
                                        G.add_edge(source_node, target_node, color='orange')

    # Get positions and colors for drawing
    pos = nx.get_node_attributes(G, 'pos')
    colors = [data['color'] for node, data in G.nodes(data=True)]
    edge_colors = [data['color'] for u, v, data in G.edges(data=True)]
    labels = nx.get_node_attributes(G, 'label')

    # Draw the graph
    # plt.figure(figsize=(12, 8))
    nx.draw(G, pos, with_labels=True, labels=labels, node_color=colors, edge_color=edge_colors)
    plt.show()

def save_data(observed_columns_dict):
    # Save observed columns to disk
    for observed_column in observed_columns_dict.values():
        observed_column.save_to_disk()

    # Save global feature neuron arrays if not lowMem
    if not lowMem:
        torch.save(global_feature_neurons_strength, feature_neurons_strength_file)
        torch.save(global_feature_neurons_permanence, feature_neurons_permanence_file)
        torch.save(global_feature_neurons_activation, feature_neurons_activation_file)

    # Save concept columns dictionary to disk
    with open(concept_columns_dict_file, 'wb') as f_out:
        pickle.dump(concept_columns_dict, f_out)

# Load the Wikipedia dataset using Hugging Face datasets
dataset = load_dataset('wikipedia', '20220301.en', split='train', streaming=True)

# Start processing the dataset
process_dataset(dataset)

