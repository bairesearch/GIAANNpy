DONE//do:
o1 preview prompt 1a5a;

v1a2x:
Please create an abstract biological neural neural network simulation for natural language processing in python. This is the specification:
- Use the pytorch API for hardware acceleration of all arrays (for efficient parallel processing of connectivity). Do not initialise the array with a predetermined dictionary; dynamically populate the arrays as new dictionary words and relations are encountered. Will be discussed below.
- Use vector graphics library such as networkx to always maintain a complete visualisation of the network (including active neurons). Only draw the parts of the network that are not empty.
- The application continuously reads sentences from a large textual corpus from huggingface (such as a Wikipedia dataset). Please use the library nltk.
- It identifies all possible POS (parts of speech) tags of each word in a sentence. Some words may have more than one possible POS tag. Please use the library spacy. 
- Each word is stored in a unique column (dictionary of columns for each word in dataset). Maintain an array of n column, each of size y, where y is the number of neurons per column. 
- Each column represents a unique concept. Columns which represent identical concepts can be merged at a later time; we will ignore column merging for now (such as "next_to" and "near"). We will also ignore the fact words can have multiple meanings for now (i.e. words can represent independent concepts).
- The columns represent general concepts, but they can also represent more specific concepts or instances by incorporating relations, such as actions (verbs) and conditions (prepositions). Examples of action relations (verbs) include "run" or eat. Examples of conditional relations (prepositions) include "near" or "above. The relations are each assigned a unique relation neuron (verb or preposition), and are connected to any number of target concept nouns (in separate columns); e.g. "bone", "to" etc
- The columns typically represent substances (or nouns), but they can also represent actions or conditions (verbs or prepositions). Only in the case of multiword prepositions (e.g. "next to") or multiword verbs (i.e. phrasal verb) (e.g. "look up") do action and condition columns have relations; e.g. "to" in "run to the park"
- Ignore all definite article ("the") and indefinite article ("a", "an") determiners when detecting qualities and action/condition relations.
- To connect relation nodes to their concept (subject) and their target (object), a natural language processor is required. Since we are simulating the creation of an abstract biological neural network, we will implement our own custom learning algorithm for associating (connecting) these nodes. Since we do not have a syntactical breakdown of sentences, we will infer that any verb/preposition that occurs after a noun word (or directly after another verb/preposition word) in a sentence is possibly connected to that word. 
- The closer the words occur in a sentence, the more likely they are to be connected, but this is not always the case. Consider an example of an exception; "the cat that ate the mouse ran to the park": in this example "ran" and "ate" are directly connected to "cat", "ran" is not directly connected to "mouse" despite being nearest to it. Therefore, we will temporarily connect every word that occurs after a column concept word in the sentence to that column concept word. 
- You should be aware this this will generate a large set of word associations for each concept word similar to historic NLP algorithms like the word co-occurance matrix. For now just instantiate every possible relation neuron for every concept neuron in the sentence. Importantly, note that the relation neurons for a given action/condition (verb/preposition) are stored in a previous concept neuron column (e.g. "dog"), not in their own concept neuron column (e.g. "ate"). E.g. a column containing concept neuron "dog" will also contain relation neurons "ran" and "ate". The relation neurons "ran" and "ate" will be connected to targets concept columns "mouse", "park".
- Generate all necessary code to run the abstract biological neural network simulation. Visualise the complete graph every time a new sentence is parsed by the application. Let the user exit the simulation at any sentence by pressing Ctrl-D.
- The columns should be visualised along the horizontal x axis (they should each be a tall rectangle). Each concept word neuron should be stored at the bottom of each column. The relation neurons should be stored along the vertical y axis of each column, above the concept neurons. Only draw the connections between the relation neurons and their target (object), not their source concept neuron (subject).  
- Please draw separate rectangles for each concept column. The concept column rectangles need to be drawn around the contents (concept and relation neurons) of each column. Ensure the neuron sizes are too large to be encapsulated by the rectangles and all the neurons (including the relation neurons) are drawin within the column rectangles.
- Every relation neuron in a concept column has an integer permanence value (initialised as 0). A relation neuron's permanence integer is increased by 3 every time the concept column neuron is activated by a sentence with that particular relation neuron being activated.  A relation neuron's permanence integer is decreased by 1 every time the concept column neuron is activated by a sentence without that particular relation neuron being activated. If a relation neuron's permanence decreases to 0, then the relation neuron is removed from the column and it will no longer be visualised.
- Print each sentence in the command line.
CHATONLY:	- Please delete the following feature; Pause the simulation after every sentence, and accept any key to continue the simulation.
CHATONLY:	- Please change the concept column integer permanence values from being initialised as 3 to being initialised as 0.
- Each word is assigned a unique concept column (this includes all pos types; even relation words are assigned a unique concept column). Please store all exclusive concept neurons POS types in a list, such that it can be easily modified (noun, proper noun, and pronoun, and unknown). 
- In addition to creating action and condition relation neurons, create relation neurons for conjunction POS types. Please store all relation word POS types in a list, such that it can be easily modified (verb, preposition, conjunction). 
- Detect quality words for every sentence word of POS type determiner, adverb, or adjective (e.g. "the", "very", or "blue) appearing before or after the current sentence concept word being processed, assuming there is no relation word between the quality word and current sentence concept word. Add associated quality neurons to the concept columns next to the relation neurons. Quality neurons are similar to relation neurons except they do not connect to a target concept column.  Please store all quality word POS types in a list, such that it can be easily modified (determiner, adverb, adjective). Note for now, quality words are assigned a unique quality neuron for every concept they are associated with, however quality neurons are only created for clarity (and programmatic	 ease). In a future implementation, qualities can be associated with concept columns by simply connecting them directly to concept column neurons.
- Draw the concept neurons blue, the action relation neurons green, the condition relation neurons red, and the quality neurons in turquoise.
- Draw a connection between every concept column representing a relation word (e.g. "eat" or "near") or quality word (e.g. "blue") to the bottom of every one of its relation or quality neuron instances within a concept column. These are concept "source" connections, and should be all drawn in blue. They communicate the semantic meaning of the particular relation or quality.
- Draw all action relation neuron target (object) connections in green, and all condition target (object) connections in red. These should be drawn from the side of the relation instance neuron and the target (object) concept neuron.
- Extend the exact same permanence rules to quality instance neurons as relation instance neurons. Every quality instance neuron in a concept column has an integer permanence value (initialised as 0). 
- Relation or quality instance nodes within a concept column represent possible features of a given concept. Connect instance nodes that occur in the same sentence together, with an instance or specific concept connection (draw as yellow). Store a permanence value for every instance or specific concept connection, using the same permanence rules used for quality or relation instance neurons.
- Relation instance nodes are connected to target column concept neurons (already implemented). Also connect relation instance nodes to every instance (i.e. feature) node in the target column that was generated or activated by the current sentence. Use the same colour scheme for target connections (conditional relations in red, and action relations in green). Store a permanence value for every relation target connection, to both target column concept neurons and target column instance neurons, using the same permanence rules used for quality or relation instance neurons and instance or specific concept connections.
- Every time a neuron or connection's permanence is increased (assuming the neuron or connection already exists), it exponentially increases its permanence from its current value. Use a squared function of its current integer permanence value (rather than a simple linear increase of +3). Initialise all permanence values to +3 (instead of 0), but do not square them when they are first initialised. Permanence still decreases linearly every time a concept column is activated by the current word but the connection or neuron within it is not referenced by the sentence (-1); already implemented so no change is required. The higher the permanence the stronger the weight for next word prediction. We are ignoring next word prediction for now, which involves an iterative topk selection of most activated neurons (similar to hopfield networks).
- Introduce an integer activation trace for every neuron and connection in the network. A neuron or connection is temporarily excited (set an activation trace value of +1) every time it is referenced by a word in a sentence. This temporary excitation lasts for 5 sentences before being removed (set an activation trace value of 0), unless it is activated by another sentence in the meantime. If it is activated by another sentence in the meantime, its activation trace is not modified (keep an activation trace value of +1), but overwrite this value in the code (set an activation trace value of +1) in case an implementation change is introduced in the future. The higher the activation trace the stronger the weight for next word prediction. We are ignoring next word prediction for now.

v1a3x:
- Place the main processing loop in a separate function, and place every for loop within the first level (i.e. lowest nesting depth) of the main processing loop in a separate function.
- Perform lemmatisation of each word in sentence before processing it in the neural network. Use a suitable python library, e.g. spacy.
CHATONLY:	- Draw the quality neurons in turquoise instead of cyan.
CHATONLY:	- Draw the quality connections (between quality neurons and their concept column neuron) in turquoise (instead of blue).
CHATONLY:	- Revert the last change "Draw the quality connections (between quality neurons and their concept column neuron) in turquoise (instead of blue)."
- Convert all possessive ending/clitic lemmas ("'s") to the "have" auxilary lemma ("have"). For example in the sentence "the student's apple", a "have" action relation will be created between "student" and "apple". "Have" auxiliary action connections are a special class of relations ('property relations') but we will not call them 'property relations' for clarity; we will call them 'action relations' in the specification, so that our code generalises (because they follow the same rules as action/condition relations). However, please draw have auxiliary action relation neurons in cyan (instead of green); their unique colour is their only difference for now.
- Create a definition connection (draw in dark blue) when a "be" auxiliary lemma is detected between two concept nodes, without an intermediary action/condition relation (e.g. "an alsatian is a black dog" will generate a definition connection between "alsatian" and "dog"). Definition connections are a special class of relation ('definition relations') but we will not call them 'definition relations' for clarity; we will call them "definition connections" in the specification, so that our code generalises (to distinguish them from action/condition relations).
- For every sentence parsed by the program, draw a projective dependency tree with the most likely semantic dependencies between lemmas in a new window. The semantic dependencies include relations (actions and conditions), qualities, and definitions. Each lemma in the sentence is assigned a node in the projective dependency graph. The projective dependency tree should draw each sentence lemma node on a horizontal line (of the same vertical height). The semantic dependency connections should be drawn as semicircular loops above them (between relevant pairs of nodes). Consider the example sentences; "the man drives the car", or "the pear is near the tree". Relation (i.e. action or condition) semantic dependencies should be drawn by a) connecting the subject concept node (e.g. "man" or "pear") and the relation instance node (e.g. "drive" or "near"), and b) connecting the relation instance node (e.g. "drive" or "near") and the target concept node (e.g. "car" or "tree") in the projective dependency tree. Consider the next example phrase; "brown rat". Quality semantic dependencies should be drawn by connecting the subject concept node (e.g. "rat") and the quality instance node (e.g. "brown"). Consider the next example sentence; "monkeys are animals". Definition semantic dependencies should be drawn by connecting the subject concept node (e.g. "monkey") and the object concept node (e.g. "animal"). Use the same colours in the projective dependency tree as the neural network for the different likely node types (concepts, actions, relations, qualities) and semantic dependency connections. Have auxiliary action dependency connections should be drawn in cyan, all other action semantic dependency conections should be drawn in green. Condition semantic dependency conections should be drawn in red. Quality semantic dependency conections should be drawn in turqoise. Definition semantic dependency conections should be drawn in dark blue. To determine the most probable semantic dependencies for each concept node in the sentence, search the existing neural network. For example (#1), consider the context "The red boat floats in the sea. The dog rows the red boat". In the second sentence the quality lemma "red" is connected to the "boat" concept and not the "eat" concept because, the network has previously encountered "red boat" in the first sentence, and connected the "red" quality to the "boat" concept node. For example (#2), consider the context "The blue flag is near the island. The flag near the island is shiny". In the second sentence the condition lemma "near" is connected to "island" and not "eat" because, the network has previously encountered "blue flag" in the first sentence, and connected the "blue" quality to the "flag" concept node.  If a most probable semantic dependency is unable to be determined, draw all possible semantic dependencies between concepts (all of the relations (actions, conditions), qualities, and definitions identified/generated in the neural network for this sentence). 
CHATONLY:	- Connect relation or quality instance nodes that b) occur in the same sentence together, with an instance or specific concept connection (draw as yellow). I.e. remove previous constraint: a) are within the same concept column.
CHATONLY:	- Ignore all definite article ("the") and indefinite article ("a", "an") determiners when detecting qualities and action/condition relations.

v1a5x:
CHATONLY:	- Revert this previous change: 'Ignore all "be" auxiliary lemmas when detecting qualities and action/condition relations.'
CHATONLY:	- Revert this previous change: 'Ignore all "do" auxiliary lemmas when detecting action relations.'

---
NOTUSED;
You have previously created code based on the following natural language specification;

Please create an abstract biological neural neural network simulation for natural language processing in python. This is the specification:
- Use the pytorch API for hardware acceleration of all arrays (for efficient parallel processing of connectivity). Do not initialise the array with a predetermined dictionary; dynamically populate the arrays as new dictionary words and relations are encountered. Will be discussed below.
- Use vector graphics library such as networkx to always maintain a complete visualisation of the network (including active neurons). Only draw the parts of the network that are not empty.
- The application continuously reads sentences from a large textual corpus from huggingface (such as a Wikipedia dataset). Please use the library nltk.
- It identifies all possible POS (parts of speech) tags of each word in a sentence. Some words may have more than one possible POS tag. Please use the library spacy. 
- Each word is stored in a unique column (dictionary of columns for each word in dataset). Maintain an array of n column, each of size y, where y is the number of neurons per column. 
- Each column represents a unique concept. Columns which represent identical concepts can be merged at a later time; we will ignore column merging for now (such as "next_to" and "near"). We will also ignore the fact words can have multiple meanings for now (i.e. words can represent independent concepts).
- The columns represent general concepts, but they can also represent more specific concepts or instances by incorporating relations, such as actions (verbs) and conditions (prepositions). Examples of action relations (verbs) include "run" or eat. Examples of conditional relations (prepositions) include "near" or "above. The relations are each assigned a unique relation neuron (verb or preposition), and are connected to any number of target concept nouns (in separate columns); e.g. "bone", "to" etc
- The columns typically represent substances (or nouns), but they can also represent actions or conditions (verbs or prepositions). Only in the case of multiword prepositions (e.g. "next to") or multiword verbs (i.e. phrasal verb) (e.g. "look up") do action and condition columns have relations; e.g. "to" in "run to the park"
- Ignore all definite article ("the") and indefinite article ("a", "an") determiners when detecting qualities and action/condition relations.
- To connect relation nodes to their concept (subject) and their target (object), a natural language processor is required. Since we are simulating the creation of an abstract biological neural network, we will implement our own custom learning algorithm for associating (connecting) these nodes. Since we do not have a syntactical breakdown of sentences, we will infer that any verb/preposition that occurs after a noun word (or directly after another verb/preposition word) in a sentence is possibly connected to that word. 
- The closer the words occur in a sentence, the more likely they are to be connected, but this is not always the case. Consider an example of an exception; "the cat that ate the mouse ran to the park": in this example "ran" and "ate" are directly connected to "cat", "ran" is not directly connected to "mouse" despite being nearest to it. Therefore, we will temporarily connect every word that occurs after a column concept word in the sentence to that column concept word. 
- You should be aware this this will generate a large set of word associations for each concept word similar to historic NLP algorithms like the word co-occurance matrix. For now just instantiate every possible relation neuron for every concept neuron in the sentence. Importantly, note that the relation neurons for a given action/condition (verb/preposition) are stored in a previous concept neuron column (e.g. "dog"), not in their own concept neuron column (e.g. "ate"). E.g. a column containing concept neuron "dog" will also contain relation neurons "ran" and "ate". The relation neurons "ran" and "ate" will be connected to targets concept columns "mouse", "park".
- Generate all necessary code to run the abstract biological neural network simulation. Visualise the complete graph every time a new sentence is parsed by the application. Let the user exit the simulation at any sentence by pressing Ctrl-D.
- The columns should be visualised along the horizontal x axis (they should each be a tall rectangle). Each concept word neuron should be stored at the bottom of each column. The relation neurons should be stored along the vertical y axis of each column, above the concept neurons. Only draw the connections between the relation neurons and their target (object), not their source concept neuron (subject).  
- Please draw separate rectangles for each concept column. The concept column rectangles need to be drawn around the contents (concept and relation neurons) of each column. Ensure the neuron sizes are too large to be encapsulated by the rectangles and all the neurons (including the relation neurons) are drawin within the column rectangles.
- Every relation neuron in a concept column has an integer permanence value (initialised as 0). A relation neuron's permanence integer is increased by 3 every time the concept column neuron is activated by a sentence with that particular relation neuron being activated.  A relation neuron's permanence integer is decreased by 1 every time the concept column neuron is activated by a sentence without that particular relation neuron being activated. If a relation neuron's permanence decreases to 0, then the relation neuron is removed from the column and it will no longer be visualised.
- Print each sentence in the command line.
- Each word is assigned a unique concept column (this includes all pos types; even relation words are assigned a unique concept column). Please store all exclusive concept neurons POS types in a list, such that it can be easily modified (noun, proper noun, and pronoun, and unknown). 
- In addition to creating action and condition relation neurons, create relation neurons for conjunction POS types. Please store all relation word POS types in a list, such that it can be easily modified (verb, preposition, conjunction). 
- Detect quality words for every sentence word of POS type determiner, adverb, or adjective (e.g. "the", "very", or "blue) appearing before or after the current sentence concept word being processed, assuming there is no relation word between the quality word and current sentence concept word. Add associated quality neurons to the concept columns next to the relation neurons. Quality neurons are similar to relation neurons except they do not connect to a target concept column.  Please store all quality word POS types in a list, such that it can be easily modified (determiner, adverb, adjective). Note for now, quality words are assigned a unique quality neuron for every concept they are associated with, however quality neurons are only created for clarity (and programmatic	 ease). In a future implementation, qualities can be associated with concept columns by simply connecting them directly to concept column neurons.
- Draw the concept neurons blue, the action relation neurons green, the condition relation neurons red, and the quality neurons in turquoise.
- Draw a connection between every concept column representing a relation word (e.g. "eat" or "near") or quality word (e.g. "blue") to the bottom of every one of its relation or quality neuron instances within a concept column. These are concept "source" connections, and should be all drawn in blue. They communicate the semantic meaning of the particular relation or quality.
- Draw all action relation neuron target (object) connections in green, and all condition target (object) connections in red. These should be drawn from the side of the relation instance neuron and the target (object) concept neuron.
- Extend the exact same permanence rules to quality instance neurons as relation instance neurons. Every quality instance neuron in a concept column has an integer permanence value (initialised as 0). 
- Relation or quality instance nodes within a concept column represent possible features of a given concept. Connect instance nodes that occur in the same sentence together, with an instance or specific concept connection (draw as yellow). Store a permanence value for every instance or specific concept connection, using the same permanence rules used for quality or relation instance neurons.
- Relation instance nodes are connected to target column concept neurons (already implemented). Also connect relation instance nodes to every instance (i.e. feature) node in the target column that was generated or activated by the current sentence. Use the same colour scheme for target connections (conditional relations in red, and action relations in green). Store a permanence value for every relation target connection, to both target column concept neurons and target column instance neurons, using the same permanence rules used for quality or relation instance neurons and instance or specific concept connections.
- Every time a neuron or connection's permanence is increased (assuming the neuron or connection already exists), it exponentially increases its permanence from its current value. Use a squared function of its current integer permanence value (rather than a simple linear increase of +3). Initialise all permanence values to +3 (instead of 0), but do not square them when they are first initialised. Permanence still decreases linearly every time a concept column is activated by the current word but the connection or neuron within it is not referenced by the sentence (-1); already implemented so no change is required. The higher the permanence the stronger the weight for next word prediction. We are ignoring next word prediction for now, which involves an iterative topk selection of most activated neurons (similar to hopfield networks).
- Introduce an integer activation trace for every neuron and connection in the network. A neuron or connection is temporarily excited (set an activation trace value of +1) every time it is referenced by a word in a sentence. This temporary excitation lasts for 5 sentences before being removed (set an activation trace value of 0), unless it is activated by another sentence in the meantime. If it is activated by another sentence in the meantime, its activation trace is not modified (keep an activation trace value of +1), but overwrite this value in the code (set an activation trace value of +1) in case an implementation change is introduced in the future. The higher the activation trace the stronger the weight for next word prediction. We are ignoring next word prediction for now.
- Place the main processing loop in a separate function, and place every for loop within the first level (i.e. lowest nesting depth) of the main processing loop in a separate function.
- Perform lemmatisation of each word in sentence before processing it in the neural network. Use a suitable python library, e.g. spacy.
- Ignore all "be" auxiliary lemmas when detecting qualities and action/condition relations. Do not assign a "be" action relation for qualities (e.g. "dogs are green"), conditions (e.g. "the cat is near the park"), or actions (e.g. "the bat is flying in the sky"). Quality neurons (e.g. "green") are directly connected to concept neurons (e.g. "dog"). Condition relations (prepositions; e.g. "near") are directly connected to their subject (e.g. "cat"). Action relations (verbs; e.g. "fly") are directly connected to their subject (e.g. "bat").
- Ignore all "do" auxiliary lemmas when detecting action relations. Do not assign a "do" action relation for actions (e.g. "the mouse does eat the cheese"). Action relations (verbs; e.g. "eat") are directly connected to their subject (e.g. "mouse").
- Convert all possessive ending/clitic lemmas ("'s") to the "have" auxilary lemma ("have"). For example in the sentence "the student's apple", a "have" action relation will be created between "student" and "apple". "Have" auxiliary action connections are a special class of relations ('property relations') but we will not call them 'property relations' for clarity; we will call them 'action relations' in the specification, so that our code generalises (because they follow the same rules as action/condition relations). However, please draw have auxiliary action relation neurons in cyan (instead of green); their unique colour is their only difference for now.
- Create a definition connection (draw in dark blue) when a "be" auxiliary lemma is detected between two concept nodes, without an intermediary action/condition relation (e.g. "an alsatian is a black dog" will generate a definition connection between "alsatian" and "dog"). Definition connections are a special class of relation ('definition relations') but we will not call them 'definition relations' for clarity; we will call them "definition connections" in the specification, so that our code generalises (to distinguish them from action/condition relations).
- For every sentence parsed by the program, draw a projective dependency tree with the most likely semantic dependencies between lemmas in a new window. The semantic dependencies include relations (actions and conditions), qualities, and definitions. Each lemma in the sentence is assigned a node in the projective dependency graph. The projective dependency tree should draw each sentence lemma node on a horizontal line (of the same vertical height). The semantic dependency connections should be drawn as semicircular loops above them (between relevant pairs of nodes). Consider the example sentences; "the man drives the car", or "the pear is near the tree". Relation (i.e. action or condition) semantic dependencies should be drawn by a) connecting the subject concept node (e.g. "man" or "pear") and the relation instance node (e.g. "drive" or "near"), and b) connecting the relation instance node (e.g. "drive" or "near") and the target concept node (e.g. "car" or "tree") in the projective dependency tree. Consider the next example phrase; "brown rat". Quality semantic dependencies should be drawn by connecting the subject concept node (e.g. "rat") and the quality instance node (e.g. "brown"). Consider the next example sentence; "monkeys are animals". Definition semantic dependencies should be drawn by connecting the subject concept node (e.g. "monkey") and the object concept node (e.g. "animal"). Use the same colours in the projective dependency tree as the neural network for the different likely node types (concepts, actions, relations, qualities) and semantic dependency connections. Have auxiliary action dependency connections should be drawn in cyan, all other action semantic dependency conections should be drawn in green. Condition semantic dependency conections should be drawn in red. Quality semantic dependency conections should be drawn in turqoise. Definition semantic dependency conections should be drawn in dark blue. To determine the most probable semantic dependencies for each concept node in the sentence, search the existing neural network. For example (#1), consider the context "The red boat floats in the sea. The dog rows the red boat". In the second sentence the quality lemma "red" is connected to the "boat" concept and not the "eat" concept because, the network has previously encountered "red boat" in the first sentence, and connected the "red" quality to the "boat" concept node. For example (#2), consider the context "The blue flag is near the island. The flag near the island is shiny". In the second sentence the condition lemma "near" is connected to "island" and not "eat" because, the network has previously encountered "blue flag" in the first sentence, and connected the "blue" quality to the "flag" concept node.  If a most probable semantic dependency is unable to be determined, draw all possible semantic dependencies between concepts (all of the relations (actions, conditions), qualities, and definitions identified/generated in the neural network for this sentence). 

Please modify the code by implementing this change:



Existing Code:
