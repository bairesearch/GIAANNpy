DONE//do:
o1 preview prompt 1a3b;

v1a2x:
Please create an abstract biological neural neural network simulation for natural language processing in python. This is the specification:
- Use the pytorch API for hardware acceleration of all arrays (for efficient parallel processing of connectivity). Do not initialise the array with a predetermined dictionary; dynamically populate the arrays as new dictionary words and relations are encountered. Will be discussed below.
- Use vector graphics library such as networkx to always maintain a complete visualisation of the network (including active neurons). Only draw the parts of the network that are not empty.
- The application continuously reads sentences from a large textual corpus from huggingface (such as a Wikipedia dataset). Please use the library nltk.
- It identifies all possible POS (parts of speech) tags of each word in a sentence. Some words may have more than one possible POS tag. Please use the library spacy. 
- Each word is stored in a unique column (dictionary of columns for each word in dataset). Maintain an array of n column, each of size y, where y is the number of neurons per column. 
- Each column represents a unique concept. Columns which represent identical concepts can be merged at a later time; we will ignore column merging for now (such as "next_to" and "near"). We will also ignore the fact words can have multiple meanings for now (i.e. words can represent independent concepts).
- The columns represent general concepts, but they can also represent more specific concepts or instances by incorporating relations, such as actions (verbs) and conditions (prepositions). Examples of action relations (verbs) include "run" or eat. Examples of conditional relations (prepositions) include "near" or "above. The relations are each assigned a unique relation neuron (verb or preposition), and are connected to any number of target concept nouns (in separate columns); e.g. "bone", "to" etc
- The columns typically represent substances (or nouns), but they can also represent actions or conditions (verbs or prepositions). Only in the case of multiword prepositions (e.g. "next to") or multiword verbs (i.e. phrasal verb) (e.g. "look up") do action and condition columns have relations; e.g. "to" in "run to the park"
- determiners are ignored for now ("the", "a" etc)
- To connect relation nodes to their concept (subject) and their target (object), a natural language processor is required. Since we are simulating the creation of an abstract biological neural network, we will implement our own custom learning algorithm for associating (connecting) these nodes. Since we do not have a syntactical breakdown of sentences, we will infer that any verb/preposition that occurs after a noun word (or directly after another verb/preposition word) in a sentence is possibly connected to that word. 
- The closer the words occur in a sentence, the more likely they are to be connected, but this is not always the case. Consider an example of an exception; "the cat that ate the mouse ran to the park": in this example "ran" and "ate" are directly connected to "cat", "ran" is not directly connected to "mouse" despite being nearest to it. Therefore, we will temporarily connect every word that occurs after a column concept word in the sentence to that column concept word. 
- You should be aware this this will generate a large set of word associations for each concept word similar to historic NLP algorithms like the word co-occurance matrix. For now just instantiate every possible relation neuron for every concept neuron in the sentence. Importantly, note that the relation neurons for a given action/condition (verb/preposition) are stored in a previous concept neuron column (e.g. "dog"), not in their own concept neuron column (e.g. "ate"). E.g. a column containing concept neuron "dog" will also contain relation neurons "ran" and "ate". The relation neurons "ran" and "ate" will be connected to targets concept columns "mouse", "park".
- Generate all necessary code to run the abstract biological neural network simulation. Visualise the complete graph every time a new sentence is parsed by the application. Let the user exit the simulation at any sentence by pressing Ctrl-D.
- The columns should be visualised along the horizontal x axis (they should each be a tall rectangle). Each concept word neuron should be stored at the bottom of each column. The relation neurons should be stored along the vertical y axis of each column, above the concept neurons. Only draw the connections between the relation neurons and their target (object), not their source concept neuron (subject).  
- Please draw separate rectangles for each concept column. The concept column rectangles need to be drawn around the contents (concept and relation neurons) of each column. Ensure the neuron sizes are too large to be encapsulated by the rectangles and all the neurons (including the relation neurons) are drawin within the column rectangles.
- Every relation neuron in a concept column has an integer permanence value (initialised as 0). A relation neuron's permanence integer is increased by 3 every time the concept column neuron is activated by a sentence with that particular relation neuron being activated.  A relation neuron's permanence integer is decreased by 1 every time the concept column neuron is activated by a sentence without that particular relation neuron being activated. If a relation neuron's permanence decreases to 0, then the relation neuron is removed from the column and it will no longer be visualised.
- Print each sentence in the command line.
CHATONLY:	- Please delete the following feature; Pause the simulation after every sentence, and accept any key to continue the simulation.
CHATONLY:	- Please change the concept column integer permanence values from being initialised as 3 to being initialised as 0.
- Each word is assigned a unique concept column (this includes all pos types; even relation words are assigned a unique concept column). Please store all exclusive concept neurons POS types in a list, such that it can be easily modified (noun, proper noun, and pronoun, and unknown). 
- In addition to creating action and condition relation neurons, create relation neurons for conjunction POS types. Please store all relation word POS types in a list, such that it can be easily modified (verb, preposition, conjunction). 
- Detect quality words for every sentence word of POS type determiner, adverb, or adjective (e.g. "the", "very", or "blue) appearing before or after the current sentence concept word being processed, assuming there is no relation word between the quality word and current sentence concept word. Add associated quality neurons to the concept columns next to the relation neurons. Quality neurons are similar to relation neurons except they do not connect to a target concept column.  Please store all quality word POS types in a list, such that it can be easily modified (determiner, adverb, adjective). Note for now, quality words are assigned a unique quality neuron for every concept they are associated with, however quality neurons are only created for clarity (and programmatic	 ease). In a future implementation, qualities can be associated with concept columns by simply connecting them directly to concept column neurons.
- Draw the concept neurons blue, the action relation neurons green, the condition relation neurons red, and the quality neurons in cyan.
- Draw a connection between every concept column representing a relation word (e.g. "eat" or "near") or quality word (e.g. "blue") to the bottom of every one of its relation or quality neuron instances within a concept column. These are concept "source" connections, and should be all drawn in blue. They communicate the semantic meaning of the particular relation or quality.
- Draw all action relation neuron target (object) connections in green, and all condition target (object) connections in red. These should be drawn from the side of the relation instance neuron and the target (object) concept neuron.
- Extend the exact same permanence rules to quality instance neurons as relation instance neurons. Every quality instance neuron in a concept column has an integer permanence value (initialised as 0). 
- Relation or quality instance nodes within a concept column represent possible features of a given concept. Connect instance nodes that a) are within the same concept column and b) occur in the same sentence together, with an instance or specific concept connection (draw as yellow). Store a permanence value for every instance or specific concept connection, using the same permanence rules used for quality or relation instance neurons.
- Relation instance nodes are connected to target column concept neurons (already implemented). Also connect relation instance nodes to every instance (i.e. feature) node in the target column that was generated or activated by the current sentence. Use the same colour scheme for target connections (conditional relations in red, and action relations in green). Store a permanence value for every relation target connection, to both target column concept neurons and target column instance neurons, using the same permanence rules used for quality or relation instance neurons and instance or specific concept connections.
- Every time a neuron or connection's permanence is increased (assuming the neuron or connection already exists), it exponentially increases its permanence from its current value. Use a squared function of its current integer permanence value (rather than a simple linear increase of +3). Initialise all permanence values to +3 (instead of 0), but do not square them when they are first initialised. Permanence still decreases linearly every time a concept column is activated by the current word but the connection or neuron within it is not referenced by the sentence (-1); already implemented so no change is required. The higher the permanence the stronger the weight for next word prediction. We are ignoring next word prediction for now, which involves an iterative topk selection of most activated neurons (similar to hopfield networks).
- Introduce an integer activation trace for every neuron and connection in the network. A neuron or connection is temporarily excited (set an activation trace value of +1) every time it is referenced by a word in a sentence. This temporary excitation lasts for 5 sentences before being removed (set an activation trace value of 0), unless it is activated by another sentence in the meantime. If it is activated by another sentence in the meantime, its activation trace is not modified (keep an activation trace value of +1), but overwrite this value in the code (set an activation trace value of +1) in case an implementation change is introduced in the future. The higher the activation trace the stronger the weight for next word prediction. We are ignoring next word prediction for now.

v1a3x:
- Place the main processing loop in a separate function, and place every for loop within the first level (i.e. lowest nesting depth) of the main processing loop in a separate function.
- Perform lemmatisation of each word in sentence before processing it in the neural network. Use a suitable python library, e.g. spacy.

