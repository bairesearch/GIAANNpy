DONE//do:
o1 preview prompt 1a4h;

v1a2x:
Please create an abstract biological neural neural network simulation for natural language processing in python. This is the specification:
- Use the pytorch API for hardware acceleration of all arrays (for efficient parallel processing of connectivity). Do not initialise the array with a predetermined dictionary; dynamically populate the arrays as new dictionary words and relations are encountered. Will be discussed below.
- Use vector graphics library such as networkx to always maintain a complete visualisation of the network (including active neurons). Only draw the parts of the network that are not empty.
- The application continuously reads sentences from a large textual corpus from huggingface (such as a Wikipedia dataset). Please use the library nltk.
- It identifies all possible POS (parts of speech) tags of each word in a sentence. Some words may have more than one possible POS tag. Please use the library spacy. 
- Each word is stored in a unique column (dictionary of columns for each word in dataset). Maintain an array of n column, each of size y, where y is the number of neurons per column. 
- Each column represents a unique concept. Columns which represent identical concepts can be merged at a later time; we will ignore column merging for now. We will also ignore the fact words can have multiple meanings for now (i.e. words can represent independent concepts).
- The columns represent general concepts, but they can also represent more specific concepts or instances by incorporating relations, such as actions (verbs) and conditions (prepositions). Examples of action relations (verbs) include "run" or eat. Examples of conditional relations (prepositions) include "near" or "above. The relations are each assigned a unique relation neuron (verb or preposition), and are connected to any number of target concept nouns (in separate columns); e.g. "bone", "to" etc
- The columns typically represent substances (or nouns), but they can also represent actions or conditions (verbs or prepositions).
- Ignore all definite article ("the") and indefinite article ("a", "an") determiners when detecting qualities and action/condition relations.
- To connect relation nodes to their concept (subject) and their target (object), a natural language processor is required. Since we are simulating the creation of an abstract biological neural network, we will implement our own custom learning algorithm for associating (connecting) these nodes. Since we do not have a syntactical breakdown of sentences, we will infer that any verb/preposition that occurs after a noun word (or directly after another verb/preposition word) in a sentence is possibly connected to that word.
- The closer the words occur in a sentence, the more likely they are to be connected, but this is not always the case. Consider an example of an exception; "the cat that ate the mouse ran to the park": in this example "ran" and "ate" are directly connected to "cat", "ran" is not directly connected to "mouse" despite being nearest to it. Therefore, we will temporarily connect every verb/preposition word that occurs after a column concept word in the sentence to that column concept neuron (or its directly preceding internal action/condition relation instance neuron).
- You should be aware this this will generate a large set of word associations for each concept word similar to historic NLP algorithms like the word co-occurance matrix. For now just instantiate every possible relation neuron for every concept neuron in the sentence. Importantly, note that the relation neurons for a given action/condition (verb/preposition) are stored in a previous concept neuron column (e.g. "dog"), not in their own concept neuron column (e.g. "ate"). E.g. a column containing concept neuron "dog" will also contain relation neurons "ate" and "to". The relation neurons "ate" and "to" will be connected to targets concept columns "mouse", "park".
- Generate all necessary code to run the abstract biological neural network simulation. Visualise the complete graph every time a new sentence is parsed by the application. Let the user exit the simulation at any sentence by pressing Ctrl-D.
- The columns should be visualised along the horizontal x axis (they should each be a tall rectangle). Each concept word neuron should be stored at the bottom of each column. The relation neurons should be stored along the vertical y axis of each column, above the concept neurons. Only draw the connections between the relation neurons and their target (object), not their source concept neuron (subject).  
- Please draw separate rectangles for each concept column. The concept column rectangles need to be drawn around the contents (concept and relation neurons) of each column. Ensure the neuron sizes are too large to be encapsulated by the rectangles and all the neurons (including the relation neurons) are drawin within the column rectangles.
- Every relation neuron in a concept column has an integer permanence value (initialised as 0). A relation neuron's permanence integer is increased by 3 every time the concept column neuron is activated by a sentence with that particular relation neuron being activated.  A relation neuron's permanence integer is decreased by 1 every time the concept column neuron is activated by a sentence without that particular relation neuron being activated. If a relation neuron's permanence decreases to 0, then the relation neuron is removed from the column and it will no longer be visualised.
- Print each sentence in the command line.
CHATONLY:	- Please delete the following feature; Pause the simulation after every sentence, and accept any key to continue the simulation.
CHATONLY:	- Please change the concept column integer permanence values from being initialised as 3 to being initialised as 0.
- Each word is assigned a unique concept column (this includes all pos types; even relation words are assigned a unique concept column). Please store all exclusive concept neurons POS types in a list, such that it can be easily modified (noun, proper noun, and pronoun, and unknown). 
- In addition to creating action and condition relation neurons, create relation neurons for conjunction POS types. Please store all relation word POS types in a list, such that it can be easily modified (verb, preposition, conjunction). 
- Detect quality words for every sentence word of POS type determiner or adjective (e.g. "the" or "blue) appearing before or after the current sentence concept word being processed, assuming there is no relation word between the quality word and current sentence concept word. Add associated quality neurons to the concept columns next to the relation neurons. Quality neurons are similar to relation neurons except they do not connect to a target concept column.  Please store all quality word POS types in a list, such that it can be easily modified (determiner, adjective). Note for now, quality words are assigned a unique quality neuron for every concept they are associated with, however quality neurons are only created for clarity (and programmatic ease).
- Draw the concept neurons blue, the action relation neurons green, the condition relation neurons red, and the quality neurons in turquoise.
- Draw a connection between every concept column representing a relation word (e.g. "eat" or "near") or quality word (e.g. "blue") to the bottom of every one of its relation or quality neuron instances within a concept column. These are concept "source" connections, and should be all drawn in blue. They communicate the semantic meaning of the particular relation or quality.
- Draw all action relation neuron target (object) connections in green, and all condition target (object) connections in red. These should be drawn from the side of the relation instance neuron and the target (object) concept neuron.
- Extend the exact same permanence rules to quality instance neurons as relation instance neurons. Every quality instance neuron in a concept column has an integer permanence value (initialised as 0). 
- Relation or quality instance nodes within a concept column represent possible features of a given concept. Connect relation or quality instance nodes that a) are within the same concept column, and b) occur in the same sentence together, with an instance or specific concept connection (draw as yellow). Store a permanence value for every instance or specific concept connection, using the same permanence rules used for quality or relation instance neurons.
- Relation instance nodes are connected to target column concept neurons (already implemented). Also connect relation instance nodes to every instance (i.e. feature) node in the target column that was generated or activated by the current sentence. Use the same colour scheme for target connections (conditional relations in red, and action relations in green). Store a permanence value for every relation target connection, to both target column concept neurons and target column instance neurons, using the same permanence rules used for quality or relation instance neurons and instance or specific concept connections.
- Every time a neuron or connection's permanence is increased (assuming the neuron or connection already exists), it exponentially increases its permanence from its current value. Use a squared function of its current integer permanence value (rather than a simple linear increase of +3). Initialise all permanence values to +3 (instead of 0), but do not square them when they are first initialised. Permanence still decreases linearly every time a concept column is activated by the current word but the connection or neuron within it is not referenced by the sentence (-1); already implemented so no change is required. The higher the permanence the stronger the weight for next word prediction. We are ignoring next word prediction for now, which involves an iterative topk selection of most activated neurons (similar to hopfield networks).
- Introduce an integer activation trace for every neuron and connection in the network. A neuron or connection is temporarily excited (set an activation trace value of +1) every time it is referenced by a word in a sentence. This temporary excitation lasts for 5 sentences before being removed (set an activation trace value of 0), unless it is activated by another sentence in the meantime. If it is activated by another sentence in the meantime, its activation trace is not modified (keep an activation trace value of +1), but overwrite this value in the code (set an activation trace value of +1) in case an implementation change is introduced in the future. The higher the activation trace the stronger the weight for next word prediction. We are ignoring next word prediction for now.

v1a3x:
- Place the main processing loop in a separate function, and place every for loop within the first level (i.e. lowest nesting depth) of the main processing loop in a separate function.
- Perform lemmatisation of each word in sentence before processing it in the neural network. Use a suitable python library, e.g. spacy.
CHATONLY:	- Draw the quality neurons in turquoise instead of cyan.
CHATONLY:	- Draw the quality connections (between quality neurons and their concept column neuron) in turquoise (instead of blue).
CHATONLY:	- Revert the last change "Draw the quality connections (between quality neurons and their concept column neuron) in turquoise (instead of blue)."
- Convert all possessive ending/clitic lemmas ("'s") to the "have" auxilary lemma ("have"). For example in the sentence "the student's apple", a "have" action relation will be created between "student" and "apple". "Have" auxiliary action connections are a special class of relations ('property relations') but we will not call them 'property relations' for clarity; we will call them 'action relations' in the specification, so that our code generalises (because they follow the same rules as action/condition relations). However, please draw have auxiliary action relation neurons in cyan (instead of green); their unique colour is their only difference for now.
- Create a definition connection (draw in dark blue) when a "be" auxiliary lemma is detected between two concept nodes, without an intermediary action/condition relation (e.g. "an alsatian is a black dog" will generate a definition connection between "alsatian" and "dog"). Definition connections are a special class of relation ('definition relations') but we will not call them 'definition relations' for clarity; we will call them "definition connections" in the specification, so that our code generalises (to distinguish them from action/condition relations).
- For every sentence parsed by the program, draw a projective dependency tree with the most likely semantic dependencies between lemmas in a new window. The semantic dependencies include relations (actions and conditions), qualities, and definitions. Each lemma in the sentence is assigned a node in the projective dependency graph. The projective dependency tree should draw each sentence lemma node on a horizontal line (of the same vertical height). The semantic dependency connections should be drawn as semicircular loops above them (between relevant pairs of nodes). Consider the example sentences; "the man drives the car", or "the pear is near the tree". Relation (i.e. action or condition) semantic dependencies should be drawn by a) connecting the subject concept node (e.g. "man" or "pear") and the relation instance node (e.g. "drive" or "near"), and b) connecting the relation instance node (e.g. "drive" or "near") and the target concept node (e.g. "car" or "tree") in the projective dependency tree. Consider the next example phrase; "brown rat". Quality semantic dependencies should be drawn by connecting the subject concept node (e.g. "rat") and the quality instance node (e.g. "brown"). Consider the next example sentence; "monkeys are animals". Definition semantic dependencies should be drawn by connecting the subject concept node (e.g. "monkey") and the object concept node (e.g. "animal"). Use the same colours in the projective dependency tree as the neural network for the different likely node types (concepts, actions, relations, qualities) and semantic dependency connections. Have auxiliary action dependency connections should be drawn in cyan, all other action semantic dependency conections should be drawn in green. Condition semantic dependency conections should be drawn in red. Quality semantic dependency conections should be drawn in turqoise. Definition semantic dependency conections should be drawn in dark blue. To determine the most probable semantic dependencies for each concept node in the sentence, search the existing neural network. For example (#1), consider the context "The red boat floats in the sea. The dog rows the red boat". In the second sentence the quality lemma "red" is connected to the "boat" concept and not the "eat" concept because, the network has previously encountered "red boat" in the first sentence, and connected the "red" quality to the "boat" concept node. For example (#2), consider the context "The blue flag is near the island. The flag near the island is shiny". In the second sentence the condition lemma "near" is connected to "island" and not "eat" because, the network has previously encountered "blue flag" in the first sentence, and connected the "blue" quality to the "flag" concept node.  If a most probable semantic dependency is unable to be determined, draw all possible semantic dependencies between concepts (all of the relations (actions, conditions), qualities, and definitions identified/generated in the neural network for this sentence). 
CHATONLY:	- Connect relation or quality instance nodes that b) occur in the same sentence together, with an instance or specific concept connection (draw as yellow). I.e. remove previous constraint: a) are within the same concept column.
CHATONLY:	- Ignore all definite article ("the") and indefinite article ("a", "an") determiners when detecting qualities and action/condition relations.

v1a4x:
CHATONLY:	- Revert this previous change: 'Ignore all "be" auxiliary lemmas when detecting qualities and action/condition relations.'
CHATONLY:	- Revert this previous change: 'Ignore all "do" auxiliary lemmas when detecting action relations.'
CHATONLY:	- Remove adverbs from the original concept quality neuron detection code (as adverbs are now assigned as feature modifiers for relations and qualities and are connected to individual relation and quality instance neurons). Only detect quality words for every sentence word of POS type determiner or adjective (e.g. "the" or "blue) [i.e. ignore adverbs] appearing before or after the current sentence concept word being processed, assuming there is no relation word between the quality word and current sentence concept word.
CHATONLY: - Restore the additional constraint "a) are within the same concept column" when creating instance or specific concept connections. Only connect relation or quality instance nodes that a) are within the same concept column, and b) occur in the same sentence together, with an instance or specific concept connection (draw as yellow).
- If a verb/preposition occurs after a noun word, and is either not in a subsequence of consecutive verbs and/or prepositions, or is last in a subsequence of consecutive verbs and/or prepositions, it is connected to that noun's concept column neuron. Whereas, if a verb/preposition word occurs directly after another verb/preposition word, it is connected to that verb/preposition's action/condition instance relation neuron (forming an internal action/condition relation). Verbs/prepositions that occur directly after another verb/preposition word are drawn in the same concept column. The connection between them is an internal action/condition relation. For example 1, "rode to" will create an internal condition relation, between the "rode" and "to" action/condition instance neurons. For example 2, "around of" will create an internal condition relation, between the "around" and "of" condition instance neurons. For example 3, "did eat" would create an internal action relation, between the "did" and "eat" action instance neurons. Internal action/condition relations are drawn in the same colours as external action/condition relations (green/red respectively).
- Introduce feature modifiers for relations and qualities. Every adverb detected adjacent a relation (verb/preposition) or quality (adjective) word is assigned a feature modifier neuron, and is connected to the relation or quality instance neuron. Feature modifier neurons and their connections are drawn in light blue, and are drawn in the same concept column as their connected relation/quality neurons. I.e. Adverbs that occur directly after or before another verb, preposition or adjective are used to identify feature modifiers for relation and quality instance neurons; feature modifier neurons are drawn in the same concept column (their neuron and connection are drawn in light blue). For example, "lived well" would create a feature modifier neuron called "well", connected to the action instance neuron "lived".

	!---
	FAIL;
	line 836, in draw_dependency_tree
    	plt.ylim(-1, max(1.5, height))  # Adjusted ylim to ensure arcs are visible
                        	  ^^^^^^
	UnboundLocalError: cannot access local variable 'height' where it is not associated with a value

---
You have previously created code based on the following natural language specification;

Please create an abstract biological neural neural network simulation for natural language processing in python. This is the specification:
- Use the pytorch API for hardware acceleration of all arrays (for efficient parallel processing of connectivity). Do not initialise the array with a predetermined dictionary; dynamically populate the arrays as new dictionary words and relations are encountered. Will be discussed below.
- Use vector graphics library such as networkx to always maintain a complete visualisation of the network (including active neurons). Only draw the parts of the network that are not empty.
- The application continuously reads sentences from a large textual corpus from huggingface (such as a Wikipedia dataset). Please use the library nltk.
- It identifies all possible POS (parts of speech) tags of each word in a sentence. Some words may have more than one possible POS tag. Please use the library spacy. 
- Each word is stored in a unique column (dictionary of columns for each word in dataset). Maintain an array of n column, each of size y, where y is the number of neurons per column. 
- Each column represents a unique concept. Columns which represent identical concepts can be merged at a later time; we will ignore column merging for now. We will also ignore the fact words can have multiple meanings for now (i.e. words can represent independent concepts).
- The columns represent general concepts, but they can also represent more specific concepts or instances by incorporating relations, such as actions (verbs) and conditions (prepositions). Examples of action relations (verbs) include "run" or eat. Examples of conditional relations (prepositions) include "near" or "above. The relations are each assigned a unique relation neuron (verb or preposition), and are connected to any number of target concept nouns (in separate columns); e.g. "bone", "to" etc
- The columns typically represent substances (or nouns), but they can also represent actions or conditions (verbs or prepositions).
- Ignore all definite article ("the") and indefinite article ("a", "an") determiners when detecting qualities and action/condition relations.
- To connect relation nodes to their concept (subject) and their target (object), a natural language processor is required. Since we are simulating the creation of an abstract biological neural network, we will implement our own custom learning algorithm for associating (connecting) these nodes. Since we do not have a syntactical breakdown of sentences, we will infer that any verb/preposition that occurs after a noun word (or directly after another verb/preposition word) in a sentence is possibly connected to that word. 
- The closer the words occur in a sentence, the more likely they are to be connected, but this is not always the case. Consider an example of an exception; "the cat that ate the mouse ran to the park": in this example "ran" and "ate" are directly connected to "cat", "ran" is not directly connected to "mouse" despite being nearest to it. Therefore, we will temporarily connect every verb/preposition word that occurs after a column concept word in the sentence to that column concept neuron (or its directly preceding internal action/condition relation instance neuron).
- You should be aware this this will generate a large set of word associations for each concept word similar to historic NLP algorithms like the word co-occurance matrix. For now just instantiate every possible relation neuron for every concept neuron in the sentence. Importantly, note that the relation neurons for a given action/condition (verb/preposition) are stored in a previous concept neuron column (e.g. "dog"), not in their own concept neuron column (e.g. "ate"). E.g. a column containing concept neuron "dog" will also contain relation neurons "ate" and "to". The relation neurons "ate" and "to" will be connected to targets concept columns "mouse", "park".
- Generate all necessary code to run the abstract biological neural network simulation. Visualise the complete graph every time a new sentence is parsed by the application. Let the user exit the simulation at any sentence by pressing Ctrl-D.
- The columns should be visualised along the horizontal x axis (they should each be a tall rectangle). Each concept word neuron should be stored at the bottom of each column. The relation neurons should be stored along the vertical y axis of each column, above the concept neurons. Only draw the connections between the relation neurons and their target (object), not their source concept neuron (subject).  
- Please draw separate rectangles for each concept column. The concept column rectangles need to be drawn around the contents (concept and relation neurons) of each column. Ensure the neuron sizes are too large to be encapsulated by the rectangles and all the neurons (including the relation neurons) are drawin within the column rectangles.
- Every relation neuron in a concept column has an integer permanence value (initialised as 0). A relation neuron's permanence integer is increased by 3 every time the concept column neuron is activated by a sentence with that particular relation neuron being activated.  A relation neuron's permanence integer is decreased by 1 every time the concept column neuron is activated by a sentence without that particular relation neuron being activated. If a relation neuron's permanence decreases to 0, then the relation neuron is removed from the column and it will no longer be visualised.
- Print each sentence in the command line.
- Each word is assigned a unique concept column (this includes all pos types; even relation words are assigned a unique concept column). Please store all exclusive concept neurons POS types in a list, such that it can be easily modified (noun, proper noun, and pronoun, and unknown). 
- In addition to creating action and condition relation neurons, create relation neurons for conjunction POS types. Please store all relation word POS types in a list, such that it can be easily modified (verb, preposition, conjunction). 
- Detect quality words for every sentence word of POS type determiner or adjective (e.g. "the" or "blue) appearing before or after the current sentence concept word being processed, assuming there is no relation word between the quality word and current sentence concept word. Add associated quality neurons to the concept columns next to the relation neurons. Quality neurons are similar to relation neurons except they do not connect to a target concept column.  Please store all quality word POS types in a list, such that it can be easily modified (determiner, adjective). Note for now, quality words are assigned a unique quality neuron for every concept they are associated with, however quality neurons are only created for clarity (and programmatic ease).
- Draw the concept neurons blue, the action relation neurons green, the condition relation neurons red, and the quality neurons in turquoise.
- Draw a connection between every concept column representing a relation word (e.g. "eat" or "near") or quality word (e.g. "blue") to the bottom of every one of its relation or quality neuron instances within a concept column. These are concept "source" connections, and should be all drawn in blue. They communicate the semantic meaning of the particular relation or quality.
- Draw all action relation neuron target (object) connections in green, and all condition target (object) connections in red. These should be drawn from the side of the relation instance neuron and the target (object) concept neuron.
- Extend the exact same permanence rules to quality instance neurons as relation instance neurons. Every quality instance neuron in a concept column has an integer permanence value (initialised as 0). 
- Relation or quality instance nodes within a concept column represent possible features of a given concept. Connect relation or quality instance nodes that a) are within the same concept column, and b) occur in the same sentence together, with an instance or specific concept connection (draw as yellow). Store a permanence value for every instance or specific concept connection, using the same permanence rules used for quality or relation instance neurons.
- Relation instance nodes are connected to target column concept neurons (already implemented). Also connect relation instance nodes to every instance (i.e. feature) node in the target column that was generated or activated by the current sentence. Use the same colour scheme for target connections (conditional relations in red, and action relations in green). Store a permanence value for every relation target connection, to both target column concept neurons and target column instance neurons, using the same permanence rules used for quality or relation instance neurons and instance or specific concept connections.
- Every time a neuron or connection's permanence is increased (assuming the neuron or connection already exists), it exponentially increases its permanence from its current value. Use a squared function of its current integer permanence value (rather than a simple linear increase of +3). Initialise all permanence values to +3 (instead of 0), but do not square them when they are first initialised. Permanence still decreases linearly every time a concept column is activated by the current word but the connection or neuron within it is not referenced by the sentence (-1); already implemented so no change is required. The higher the permanence the stronger the weight for next word prediction. We are ignoring next word prediction for now, which involves an iterative topk selection of most activated neurons (similar to hopfield networks).
- Introduce an integer activation trace for every neuron and connection in the network. A neuron or connection is temporarily excited (set an activation trace value of +1) every time it is referenced by a word in a sentence. This temporary excitation lasts for 5 sentences before being removed (set an activation trace value of 0), unless it is activated by another sentence in the meantime. If it is activated by another sentence in the meantime, its activation trace is not modified (keep an activation trace value of +1), but overwrite this value in the code (set an activation trace value of +1) in case an implementation change is introduced in the future. The higher the activation trace the stronger the weight for next word prediction. We are ignoring next word prediction for now.
- Place the main processing loop in a separate function, and place every for loop within the first level (i.e. lowest nesting depth) of the main processing loop in a separate function.
- Perform lemmatisation of each word in sentence before processing it in the neural network. Use a suitable python library, e.g. spacy.
- Convert all possessive ending/clitic lemmas ("'s") to the "have" auxilary lemma ("have"). For example in the sentence "the student's apple", a "have" action relation will be created between "student" and "apple". "Have" auxiliary action connections are a special class of relations ('property relations') but we will not call them 'property relations' for clarity; we will call them 'action relations' in the specification, so that our code generalises (because they follow the same rules as action/condition relations). However, please draw have auxiliary action relation neurons in cyan (instead of green); their unique colour is their only difference for now.
- Create a definition connection (draw in dark blue) when a "be" auxiliary lemma is detected between two concept nodes, without an intermediary action/condition relation (e.g. "an alsatian is a black dog" will generate a definition connection between "alsatian" and "dog"). Definition connections are a special class of relation ('definition relations') but we will not call them 'definition relations' for clarity; we will call them "definition connections" in the specification, so that our code generalises (to distinguish them from action/condition relations).
- For every sentence parsed by the program, draw a projective dependency tree with the most likely semantic dependencies between lemmas in a new window. The semantic dependencies include relations (actions and conditions), qualities, and definitions. Each lemma in the sentence is assigned a node in the projective dependency graph. The projective dependency tree should draw each sentence lemma node on a horizontal line (of the same vertical height). The semantic dependency connections should be drawn as semicircular loops above them (between relevant pairs of nodes). Consider the example sentences; "the man drives the car", or "the pear is near the tree". Relation (i.e. action or condition) semantic dependencies should be drawn by a) connecting the subject concept node (e.g. "man" or "pear") and the relation instance node (e.g. "drive" or "near"), and b) connecting the relation instance node (e.g. "drive" or "near") and the target concept node (e.g. "car" or "tree") in the projective dependency tree. Consider the next example phrase; "brown rat". Quality semantic dependencies should be drawn by connecting the subject concept node (e.g. "rat") and the quality instance node (e.g. "brown"). Consider the next example sentence; "monkeys are animals". Definition semantic dependencies should be drawn by connecting the subject concept node (e.g. "monkey") and the object concept node (e.g. "animal"). Use the same colours in the projective dependency tree as the neural network for the different likely node types (concepts, actions, relations, qualities) and semantic dependency connections. Have auxiliary action dependency connections should be drawn in cyan, all other action semantic dependency conections should be drawn in green. Condition semantic dependency conections should be drawn in red. Quality semantic dependency conections should be drawn in turqoise. Definition semantic dependency conections should be drawn in dark blue. To determine the most probable semantic dependencies for each concept node in the sentence, search the existing neural network. For example (#1), consider the context "The red boat floats in the sea. The dog rows the red boat". In the second sentence the quality lemma "red" is connected to the "boat" concept and not the "eat" concept because, the network has previously encountered "red boat" in the first sentence, and connected the "red" quality to the "boat" concept node. For example (#2), consider the context "The blue flag is near the island. The flag near the island is shiny". In the second sentence the condition lemma "near" is connected to "island" and not "eat" because, the network has previously encountered "blue flag" in the first sentence, and connected the "blue" quality to the "flag" concept node.  If a most probable semantic dependency is unable to be determined, draw all possible semantic dependencies between concepts (all of the relations (actions, conditions), qualities, and definitions identified/generated in the neural network for this sentence). 
- If a verb/preposition occurs after a noun word, and is either not in a subsequence of consecutive verbs and/or prepositions, or is last in a subsequence of consecutive verbs and/or prepositions, it is connected to that noun's concept column neuron. Whereas, if a verb/preposition word occurs directly after another verb/preposition word, it is connected to that verb/preposition's action/condition instance relation neuron (forming an internal action/condition relation). Verbs/prepositions that occur directly after another verb/preposition word are drawn in the same concept column. The connection between them is an internal action/condition relation. For example 1, "rode to" will create an internal condition relation, between the "rode" and "to" action/condition instance neurons. For example 2, "around of" will create an internal condition relation, between the "around" and "of" condition instance neurons. For example 3, "did eat" would create an internal action relation, between the "did" and "eat" action instance neurons. Internal action/condition relations are drawn in the same colours as external action/condition relations (green/red respectively).

Please modify the code by implementing these changes:
- Introduce feature modifiers for relations and qualities. Every adverb detected adjacent a relation (verb/preposition) or quality (adjective) word is assigned a feature modifier neuron, and is connected to the relation or quality instance neuron. Feature modifier neurons and their connections are drawn in light blue, and are drawn in the same concept column as their connected relation/quality neurons. I.e. Adverbs that occur directly after or before another verb, preposition or adjective are used to identify feature modifiers for relation and quality instance neurons; feature modifier neurons are drawn in the same concept column (their neuron and connection are drawn in light blue). For example, "lived well" would create a feature modifier neuron called "well", connected to the action instance neuron "lived".


Existing Code:
import torch
import networkx as nx
import matplotlib.pyplot as plt
import matplotlib.patches
import nltk
import spacy
from nltk.tokenize import sent_tokenize
from datasets import load_dataset

# Ensure necessary NLTK data packages are downloaded
nltk.download('punkt')

# Load spaCy English model
nlp = spacy.load('en_core_web_sm')

# Load dataset from Hugging Face
dataset = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train')

# Get text data
text = '\n'.join(dataset['text'])

# Tokenize text into sentences
sentences = sent_tokenize(text)

# Initialize the neural network representation
columns = {}  # key: lemma (concept), value: {'concept_neuron', 'permanence', 'relation_neurons', 'quality_neurons', 'instance_connections'}

# Initialize NetworkX graph
G = nx.Graph()

# Global neuron ID counter to ensure unique IDs
neuron_id_counter = 0

# Sentence counter to manage activation traces
sentence_counter = 0

# Lists for POS types
concept_pos_list = ['NOUN', 'PROPN', 'PRON', 'X']  # POS types for concept columns
relation_pos_list = ['VERB', 'ADP', 'CONJ']        # POS types for relation neurons
quality_pos_list = ['DET', 'ADJ']                  # Removed 'ADV' from the list

# Function to visualize the network
def visualize_network(G, columns):
    plt.figure(figsize=(14, 10))
    pos = {}  # positions of nodes
    labels = {}  # labels of nodes
    x_margin = 2  # margin between columns
    y_margin = 1  # margin between neurons in column
    neuron_size = 500
    x_positions = {}
    max_y = 0

    # Draw concept columns
    for i, (concept_lemma, neurons) in enumerate(columns.items()):
        # x position is i * x_margin
        x = i * x_margin * 2
        x_positions[concept_lemma] = x
        # Concept neuron at bottom (y=0)
        concept_neuron_id = neurons['concept_neuron']
        pos[concept_neuron_id] = (x, 0)
        labels[concept_neuron_id] = concept_lemma
        # Relation neurons above
        relation_y_positions = {}
        for j, (relation_lemma, relation_info) in enumerate(neurons['relation_neurons'].items()):
            relation_neuron_id = relation_info['neuron_id']
            y = (j + 1) * y_margin * 2  # y position for relation neurons
            pos[relation_neuron_id] = (x, y)
            labels[relation_neuron_id] = relation_lemma
            relation_y_positions[relation_neuron_id] = y
            if y > max_y:
                max_y = y
        # Quality neurons above relation neurons
        for k, (quality_lemma, quality_info) in enumerate(neurons['quality_neurons'].items()):
            quality_neuron_id = quality_info['neuron_id']
            y = (len(neurons['relation_neurons']) + k + 1) * y_margin * 2
            pos[quality_neuron_id] = (x, y)
            labels[quality_neuron_id] = quality_lemma
            if y > max_y:
                max_y = y

    # Draw nodes
    node_colors = []
    for node_id in G.nodes():
        if node_id in [neurons['concept_neuron'] for neurons in columns.values()]:
            node_colors.append('blue')  # Concept neurons
        elif any(node_id in [info['neuron_id'] for info in neurons['relation_neurons'].values()] for neurons in columns.values()):
            # Relation neurons
            for neurons in columns.values():
                for relation_lemma, relation_info in neurons['relation_neurons'].items():
                    if relation_info['neuron_id'] == node_id:
                        relation_pos = relation_info['pos']
                        relation_lemma = relation_info['lemma']
                        if relation_lemma == 'have':
                            node_colors.append('cyan')  # 'have' auxiliary action relations
                        elif relation_pos == 'VERB':
                            node_colors.append('green')  # Action
                        elif relation_pos == 'ADP':
                            node_colors.append('red')    # Condition
                        elif relation_pos == 'CONJ':
                            node_colors.append('green')  # Conjunctions colored as action
                        else:
                            node_colors.append('gray')
                        break
        elif any(node_id in [info['neuron_id'] for info in neurons['quality_neurons'].values()] for neurons in columns.values()):
            node_colors.append('turquoise')  # Quality neurons
        else:
            node_colors.append('gray')

    # Draw edges
    edge_colors = []
    edge_styles = []
    for u, v in G.edges():
        edge = G.get_edge_data(u, v)
        if edge['type'] == 'relation_target':
            if edge.get('lemma') == 'have':
                edge_colors.append('cyan')
            elif edge['pos'] == 'VERB':
                edge_colors.append('green')
            elif edge['pos'] == 'ADP':
                edge_colors.append('red')
            elif edge['pos'] == 'CONJ':
                edge_colors.append('green')
            else:
                edge_colors.append('gray')
            edge_styles.append('solid')
        elif edge['type'] == 'concept_source':
            edge_colors.append('blue')  # All concept_source edges are colored blue
            edge_styles.append('solid')
        elif edge['type'] == 'definition':
            edge_colors.append('darkblue')  # Definition connections
            edge_styles.append('solid')
        elif edge['type'] == 'instance_connection':
            edge_colors.append('yellow')
            edge_styles.append('solid')
        elif edge['type'] == 'internal_relation':
            if edge.get('pos') == 'VERB':
                edge_colors.append('green')
            elif edge.get('pos') == 'ADP':
                edge_colors.append('red')
            elif edge.get('pos') == 'CONJ':
                edge_colors.append('green')
            else:
                edge_colors.append('gray')
            edge_styles.append('dashed')
        else:
            edge_colors.append('gray')
            edge_styles.append('solid')

    nx.draw_networkx_nodes(G, pos, node_size=neuron_size, node_color=node_colors)
    nx.draw_networkx_edges(G, pos, edge_color=edge_colors, style=edge_styles)
    nx.draw_networkx_labels(G, pos, labels, font_size=8)

    # Draw rectangles around columns
    for concept_lemma, x in x_positions.items():
        # Get y positions of neurons in this column
        neurons = columns[concept_lemma]
        y_positions = [pos[neurons['concept_neuron']][1]]
        for relation_info in neurons['relation_neurons'].values():
            if relation_info['column'] == concept_lemma:
                y_positions.append(pos[relation_info['neuron_id']][1])
        for quality_info in neurons['quality_neurons'].values():
            y_positions.append(pos[quality_info['neuron_id']][1])
        y_min = min(y_positions) - y_margin
        y_max = max(y_positions) + y_margin
        # Draw rectangle
        plt.gca().add_patch(plt.Rectangle((x - x_margin, y_min), x_margin * 2, y_max - y_min + y_margin, fill=False, edgecolor='black'))
    plt.axis('off')
    plt.show()

def ensure_words_in_columns(lemmas, tokens):
    global neuron_id_counter
    for lemma, token in zip(lemmas, tokens):
        pos_tag = token.pos_
        dep_tag = token.dep_
        tag = token.tag_

        # Ignore "be" and "do" auxiliaries
        if dep_tag == 'aux' and lemma in ['be', 'do']:
            continue

        # Convert possessive clitic to "have"
        if tag == 'POS':
            lemma = 'have'
            pos_tag = 'VERB'
            dep_tag = 'aux'

        if not lemma.isalpha():
            continue
        if lemma not in columns:
            neuron_id_counter += 1
            concept_neuron_id = neuron_id_counter
            columns[lemma] = {
                'concept_neuron': concept_neuron_id,
                'permanence': 3,  # Initialized to 3
                'concept_activation_trace_counter': 0,  # Activation trace counter
                'relation_neurons': {},
                'quality_neurons': {},
                'instance_connections': {}
            }
            # Add the concept neuron to the graph
            G.add_node(concept_neuron_id)

def collect_qualities_and_initialize_instances(lemmas, pos_tags, tokens):
    global neuron_id_counter
    activated_concepts = {}
    activated_instances = {}
    activated_qualities = {}
    for idx, (lemma, pos_tag) in enumerate(zip(lemmas, pos_tags)):
        if not lemma.isalpha():
            continue

        activated_concepts[lemma] = True

        # Set activation trace for concept neuron
        columns[lemma]['concept_activation_trace_counter'] = 5  # Reset to 5

        # Collect qualities for this lemma
        qualities_found = []
        # Check previous lemma
        if idx > 0:
            prev_lemma = lemmas[idx - 1]
            prev_pos_tag = pos_tags[idx - 1]
            if prev_pos_tag in quality_pos_list:
                intervening_pos = pos_tags[idx - 1:idx]
                if not any(pos in relation_pos_list for pos in intervening_pos):
                    qualities_found.append(prev_lemma)
                    # Ensure the quality neuron exists in the column
                    if prev_lemma not in columns[lemma]['quality_neurons']:
                        neuron_id_counter += 1
                        quality_neuron_id = neuron_id_counter
                        columns[lemma]['quality_neurons'][prev_lemma] = {
                            'neuron_id': quality_neuron_id,
                            'permanence': 3,  # Initialized to 3
                            'activation_trace_counter': 5,  # Activation trace counter
                            'pos': prev_pos_tag,
                            'first_activation': True
                        }
                        # Add the quality neuron to the graph
                        G.add_node(quality_neuron_id)
                        # Ensure prev_lemma has a concept neuron
                        ensure_words_in_columns([prev_lemma], [tokens[idx - 1]])
                        prev_lemma_concept_id = columns[prev_lemma]['concept_neuron']
                        # Draw concept source connection
                        G.add_edge(prev_lemma_concept_id, quality_neuron_id, type='concept_source')
                    else:
                        quality_neuron_id = columns[lemma]['quality_neurons'][prev_lemma]['neuron_id']
                        # Reset activation trace counter
                        columns[lemma]['quality_neurons'][prev_lemma]['activation_trace_counter'] = 5
        # Check next lemma
        if idx < len(lemmas) - 1:
            next_lemma = lemmas[idx + 1]
            next_pos_tag = pos_tags[idx + 1]
            if next_pos_tag in quality_pos_list:
                intervening_pos = pos_tags[idx + 1:idx + 2]
                if not any(pos in relation_pos_list for pos in intervening_pos):
                    qualities_found.append(next_lemma)
                    if next_lemma not in columns[lemma]['quality_neurons']:
                        neuron_id_counter += 1
                        quality_neuron_id = neuron_id_counter
                        columns[lemma]['quality_neurons'][next_lemma] = {
                            'neuron_id': quality_neuron_id,
                            'permanence': 3,  # Initialized to 3
                            'activation_trace_counter': 5,  # Activation trace counter
                            'pos': next_pos_tag,
                            'first_activation': True
                        }
                        # Add the quality neuron to the graph
                        G.add_node(quality_neuron_id)
                        # Ensure next_lemma has a concept neuron
                        ensure_words_in_columns([next_lemma], [tokens[idx + 1]])
                        next_lemma_concept_id = columns[next_lemma]['concept_neuron']
                        # Draw concept source connection
                        G.add_edge(next_lemma_concept_id, quality_neuron_id, type='concept_source')
                    else:
                        quality_neuron_id = columns[lemma]['quality_neurons'][next_lemma]['neuron_id']
                        # Reset activation trace counter
                        columns[lemma]['quality_neurons'][next_lemma]['activation_trace_counter'] = 5

        # Store the qualities found for this concept in this sentence
        activated_qualities[lemma] = qualities_found

        # Collect instance neurons for this word
        instance_neurons = []
        # Quality neurons
        for qual_lemma in qualities_found:
            if qual_lemma in columns[lemma]['quality_neurons']:
                qual_neuron_info = columns[lemma]['quality_neurons'][qual_lemma]
                qual_neuron_id = qual_neuron_info['neuron_id']
                instance_neurons.append(qual_neuron_id)
                # Reset activation trace counter
                qual_neuron_info['activation_trace_counter'] = 5

        # Store activated instance neurons for this concept
        activated_instances[lemma] = instance_neurons

    return activated_concepts, activated_instances, activated_qualities

def process_relations(lemmas, pos_tags, tokens, activated_instances):
    global neuron_id_counter
    activated_relations = {}
    activated_relation_targets = {}
    for idx, (lemma, pos_tag) in enumerate(zip(lemmas, pos_tags)):
        if not lemma.isalpha():
            continue

        if pos_tag not in concept_pos_list:
            continue

        # Start processing relations after the concept word
        curr_idx = idx + 1
        relation_sequence = []
        while curr_idx < len(lemmas):
            next_lemma = lemmas[curr_idx]
            next_pos_tag = pos_tags[curr_idx]
            if next_pos_tag in relation_pos_list:
                # Ignore "be" and "do" auxiliaries
                if next_lemma in ['be', 'do'] and tokens[curr_idx].dep_ == 'aux':
                    curr_idx += 1
                    continue
                relation_sequence.append(curr_idx)
                curr_idx += 1
            else:
                break

        # Process the relation sequence
        if not relation_sequence:
            continue

        # Initialize current_column_lemma as the current concept lemma
        current_column_lemma = lemma

        for i in range(len(relation_sequence)):
            relation_idx = relation_sequence[i]
            relation_lemma = lemmas[relation_idx]
            relation_pos_tag = pos_tags[relation_idx]
            relation_token = tokens[relation_idx]

            # Determine the concept column to which this relation neuron belongs
            if i == 0:
                # First relation word after the noun
                if len(relation_sequence) == 1:
                    # Only one relation word, connect to noun's concept neuron
                    concept_lemma = lemma
                    # Ensure the relation neuron exists in the noun's concept column
                    if relation_lemma not in columns[concept_lemma]['relation_neurons']:
                        neuron_id_counter += 1
                        relation_neuron_id = neuron_id_counter
                        columns[concept_lemma]['relation_neurons'][relation_lemma] = {
                            'neuron_id': relation_neuron_id,
                            'permanence': 3,
                            'activation_trace_counter': 5,
                            'pos': relation_pos_tag,
                            'lemma': relation_lemma,
                            'target_connections': {},
                            'first_activation': True,
                            'column': concept_lemma  # Store the column it belongs to
                        }
                        G.add_node(relation_neuron_id)
                        # Draw concept source connection
                        ensure_words_in_columns([relation_lemma], [relation_token])
                        relation_lemma_concept_id = columns[relation_lemma]['concept_neuron']
                        columns[relation_lemma]['concept_activation_trace_counter'] = 5
                        G.add_edge(relation_lemma_concept_id, relation_neuron_id, type='concept_source')
                        # Connect to noun's concept neuron
                        concept_neuron_id = columns[concept_lemma]['concept_neuron']
                        G.add_edge(concept_neuron_id, relation_neuron_id, type='relation_target', pos=relation_pos_tag, lemma=relation_lemma)
                    else:
                        relation_neuron_info = columns[concept_lemma]['relation_neurons'][relation_lemma]
                        relation_neuron_id = relation_neuron_info['neuron_id']
                        # Reset activation trace counter
                        relation_neuron_info['activation_trace_counter'] = 5
                        # Connect to noun's concept neuron
                        concept_neuron_id = columns[concept_lemma]['concept_neuron']
                        G.add_edge(concept_neuron_id, relation_neuron_id, type='relation_target', pos=relation_pos_tag, lemma=relation_lemma)
                else:
                    # More than one relation word, connect to next relation word
                    next_relation_idx = relation_sequence[i + 1]
                    next_relation_lemma = lemmas[next_relation_idx]
                    next_relation_pos_tag = pos_tags[next_relation_idx]
                    next_relation_token = tokens[next_relation_idx]

                    # Ensure current relation neuron exists in columns
                    if relation_lemma not in columns[current_column_lemma]['relation_neurons']:
                        neuron_id_counter += 1
                        relation_neuron_id = neuron_id_counter
                        columns[current_column_lemma]['relation_neurons'][relation_lemma] = {
                            'neuron_id': relation_neuron_id,
                            'permanence': 3,
                            'activation_trace_counter': 5,
                            'pos': relation_pos_tag,
                            'lemma': relation_lemma,
                            'target_connections': {},
                            'first_activation': True,
                            'column': current_column_lemma
                        }
                        G.add_node(relation_neuron_id)
                        # Draw concept source connection
                        ensure_words_in_columns([relation_lemma], [relation_token])
                        relation_lemma_concept_id = columns[relation_lemma]['concept_neuron']
                        columns[relation_lemma]['concept_activation_trace_counter'] = 5
                        G.add_edge(relation_lemma_concept_id, relation_neuron_id, type='concept_source')
                    else:
                        relation_neuron_info = columns[current_column_lemma]['relation_neurons'][relation_lemma]
                        relation_neuron_id = relation_neuron_info['neuron_id']
                        relation_neuron_info['activation_trace_counter'] = 5

                    # Ensure next relation neuron exists in columns
                    if next_relation_lemma not in columns[current_column_lemma]['relation_neurons']:
                        neuron_id_counter += 1
                        next_relation_neuron_id = neuron_id_counter
                        columns[current_column_lemma]['relation_neurons'][next_relation_lemma] = {
                            'neuron_id': next_relation_neuron_id,
                            'permanence': 3,
                            'activation_trace_counter': 5,
                            'pos': next_relation_pos_tag,
                            'lemma': next_relation_lemma,
                            'target_connections': {},
                            'first_activation': True,
                            'column': current_column_lemma
                        }
                        G.add_node(next_relation_neuron_id)
                        # Draw concept source connection
                        ensure_words_in_columns([next_relation_lemma], [next_relation_token])
                        next_relation_lemma_concept_id = columns[next_relation_lemma]['concept_neuron']
                        columns[next_relation_lemma]['concept_activation_trace_counter'] = 5
                        G.add_edge(next_relation_lemma_concept_id, next_relation_neuron_id, type='concept_source')
                    else:
                        next_relation_neuron_info = columns[current_column_lemma]['relation_neurons'][next_relation_lemma]
                        next_relation_neuron_id = next_relation_neuron_info['neuron_id']
                        next_relation_neuron_info['activation_trace_counter'] = 5

                    # Connect current relation to next relation (internal action/condition relation)
                    G.add_edge(relation_neuron_id, next_relation_neuron_id, type='internal_relation', pos=relation_pos_tag)
                    # Update current_column_lemma to the current relation lemma
                    current_column_lemma = current_column_lemma
            else:
                # Subsequent relation words
                previous_relation_idx = relation_sequence[i - 1]
                previous_relation_lemma = lemmas[previous_relation_idx]
                previous_relation_pos_tag = pos_tags[previous_relation_idx]
                previous_relation_token = tokens[previous_relation_idx]

                # Ensure current relation neuron exists in columns
                if relation_lemma not in columns[current_column_lemma]['relation_neurons']:
                    neuron_id_counter += 1
                    relation_neuron_id = neuron_id_counter
                    columns[current_column_lemma]['relation_neurons'][relation_lemma] = {
                        'neuron_id': relation_neuron_id,
                        'permanence': 3,
                        'activation_trace_counter': 5,
                        'pos': relation_pos_tag,
                        'lemma': relation_lemma,
                        'target_connections': {},
                        'first_activation': True,
                        'column': current_column_lemma
                    }
                    G.add_node(relation_neuron_id)
                    # Draw concept source connection
                    ensure_words_in_columns([relation_lemma], [relation_token])
                    relation_lemma_concept_id = columns[relation_lemma]['concept_neuron']
                    columns[relation_lemma]['concept_activation_trace_counter'] = 5
                    G.add_edge(relation_lemma_concept_id, relation_neuron_id, type='concept_source')
                else:
                    relation_neuron_info = columns[current_column_lemma]['relation_neurons'][relation_lemma]
                    relation_neuron_id = relation_neuron_info['neuron_id']
                    relation_neuron_info['activation_trace_counter'] = 5

                # Ensure previous relation neuron exists
                previous_relation_neuron_info = columns[current_column_lemma]['relation_neurons'][previous_relation_lemma]
                previous_relation_neuron_id = previous_relation_neuron_info['neuron_id']

                # Connect previous relation to current relation (internal action/condition relation)
                G.add_edge(previous_relation_neuron_id, relation_neuron_id, type='internal_relation', pos=relation_pos_tag)

                if i == len(relation_sequence) - 1:
                    # Last relation word, connect to noun's concept neuron
                    concept_neuron_id = columns[lemma]['concept_neuron']
                    G.add_edge(concept_neuron_id, relation_neuron_id, type='relation_target', pos=relation_pos_tag, lemma=relation_lemma)
                else:
                    # Connect current relation to next relation
                    next_relation_idx = relation_sequence[i + 1]
                    next_relation_lemma = lemmas[next_relation_idx]
                    next_relation_pos_tag = pos_tags[next_relation_idx]
                    next_relation_token = tokens[next_relation_idx]

                    # Ensure next relation neuron exists in columns
                    if next_relation_lemma not in columns[current_column_lemma]['relation_neurons']:
                        neuron_id_counter += 1
                        next_relation_neuron_id = neuron_id_counter
                        columns[current_column_lemma]['relation_neurons'][next_relation_lemma] = {
                            'neuron_id': next_relation_neuron_id,
                            'permanence': 3,
                            'activation_trace_counter': 5,
                            'pos': next_relation_pos_tag,
                            'lemma': next_relation_lemma,
                            'target_connections': {},
                            'first_activation': True,
                            'column': current_column_lemma
                        }
                        G.add_node(next_relation_neuron_id)
                        # Draw concept source connection
                        ensure_words_in_columns([next_relation_lemma], [next_relation_token])
                        next_relation_lemma_concept_id = columns[next_relation_lemma]['concept_neuron']
                        columns[next_relation_lemma]['concept_activation_trace_counter'] = 5
                        G.add_edge(next_relation_lemma_concept_id, next_relation_neuron_id, type='concept_source')
                    else:
                        next_relation_neuron_info = columns[current_column_lemma]['relation_neurons'][next_relation_lemma]
                        next_relation_neuron_id = next_relation_neuron_info['neuron_id']
                        next_relation_neuron_info['activation_trace_counter'] = 5

                    # Connect current relation to next relation (internal action/condition relation)
                    G.add_edge(relation_neuron_id, next_relation_neuron_id, type='internal_relation', pos=relation_pos_tag)

    return activated_relations, activated_relation_targets

def process_definitions(tokens):
    lemmas = [token.lemma_ for token in tokens]
    pos_tags = [token.pos_ for token in tokens]
    dep_tags = [token.dep_ for token in tokens]

    for idx, token in enumerate(tokens):
        lemma = token.lemma_
        pos_tag = token.pos_
        if not lemma.isalpha():
            continue
        # Check if current lemma is a concept lemma
        if pos_tag in concept_pos_list:
            # Look ahead for 'be' auxiliary lemma
            for next_idx in range(idx+1, len(tokens)):
                next_token = tokens[next_idx]
                next_lemma = next_token.lemma_
                next_pos_tag = next_token.pos_
                if not next_lemma.isalpha():
                    continue
                if next_pos_tag in concept_pos_list:
                    # Found another concept lemma without an intermediary action/condition relation
                    # Now check if there was a 'be' auxiliary lemma in between
                    be_found = False
                    intermediary_found = False
                    for in_between_idx in range(idx+1, next_idx):
                        in_between_token = tokens[in_between_idx]
                        in_between_lemma = in_between_token.lemma_
                        in_between_pos = in_between_token.pos_
                        in_between_dep = in_between_token.dep_
                        if in_between_lemma == 'be' and in_between_dep == 'aux':
                            be_found = True
                        elif in_between_pos in relation_pos_list:
                            intermediary_found = True
                            break
                    if be_found and not intermediary_found:
                        # Create definition connection between lemma and next_lemma
                        concept_neuron_id_1 = columns[lemma]['concept_neuron']
                        concept_neuron_id_2 = columns[next_lemma]['concept_neuron']
                        # Add edge if not already present
                        if not G.has_edge(concept_neuron_id_1, concept_neuron_id_2):
                            G.add_edge(concept_neuron_id_1, concept_neuron_id_2, type='definition')
                        break  # Only connect to the first concept word satisfying the condition
                    elif intermediary_found:
                        break  # An intermediary relation is found, so break
                elif next_pos_tag in relation_pos_list:
                    break  # An intermediary action/condition relation is found, so break
                else:
                    continue

def update_instance_connections(activated_instances):
    for lemma, instance_neurons in activated_instances.items():
        if 'instance_connections' not in columns[lemma]:
            columns[lemma]['instance_connections'] = {}
        if len(instance_neurons) >= 2:
            for i in range(len(instance_neurons)):
                for j in range(i + 1, len(instance_neurons)):
                    neuron_pair = tuple(sorted((instance_neurons[i], instance_neurons[j])))
                    if neuron_pair not in columns[lemma]['instance_connections']:
                        columns[lemma]['instance_connections'][neuron_pair] = {
                            'permanence': 3,  # Initialized to 3
                            'activation_trace_counter': 5,  # Activation trace counter
                            'first_activation': True
                        }
                        # Add edge to graph if not already present
                        if not G.has_edge(neuron_pair[0], neuron_pair[1]):
                            G.add_edge(neuron_pair[0], neuron_pair[1], type='instance_connection')
                    else:
                        # Reset activation trace counter
                        columns[lemma]['instance_connections'][neuron_pair]['activation_trace_counter'] = 5

def update_permanence_values_concept_neurons(activated_concepts):
    for concept_lemma, neurons in columns.items():
        if concept_lemma in activated_concepts:
            neurons['permanence'] += 1  # No change required

def update_permanence_values_relation_neurons(activated_concepts, activated_relations, activated_relation_targets):
    for concept_lemma, neurons in columns.items():
        if concept_lemma in activated_concepts:
            active_relations = activated_relations.get(concept_lemma, [])
            for relation_lemma, relation_info in list(neurons['relation_neurons'].items()):
                relation_neuron_id = relation_info['neuron_id']
                if relation_lemma in active_relations:
                    if relation_info.get('first_activation', False):
                        relation_info['first_activation'] = False
                    else:
                        relation_info['permanence'] = relation_info['permanence'] ** 2
                    # Update target connections
                    if 'target_connections' in relation_info:
                        for target_neuron_id, target_conn_info in list(relation_info['target_connections'].items()):
                            if relation_neuron_id in activated_relation_targets and target_neuron_id in activated_relation_targets[relation_neuron_id]:
                                if target_conn_info.get('first_activation', False):
                                    target_conn_info['first_activation'] = False
                                else:
                                    target_conn_info['permanence'] = target_conn_info['permanence'] ** 2
                            else:
                                target_conn_info['permanence'] -= 1
                                if target_conn_info['permanence'] <= 0:
                                    # Remove edge
                                    if G.has_edge(relation_neuron_id, target_neuron_id):
                                        G.remove_edge(relation_neuron_id, target_neuron_id)
                                    del relation_info['target_connections'][target_neuron_id]
                else:
                    # Decrease permanence by 1
                    relation_info['permanence'] -= 1
                    if relation_info['permanence'] <= 0:
                        # Remove relation neuron
                        if G.has_node(relation_neuron_id):
                            G.remove_node(relation_neuron_id)
                        del neurons['relation_neurons'][relation_lemma]
                    else:
                        # Decrease permanence of target connections
                        if 'target_connections' in relation_info:
                            for target_neuron_id, target_conn_info in list(relation_info['target_connections'].items()):
                                target_conn_info['permanence'] -= 1
                                if target_conn_info['permanence'] <= 0:
                                    # Remove edge
                                    if G.has_edge(relation_neuron_id, target_neuron_id):
                                        G.remove_edge(relation_neuron_id, target_neuron_id)
                                    del relation_info['target_connections'][target_neuron_id]

def update_permanence_values_quality_neurons(activated_concepts, activated_qualities):
    for concept_lemma, neurons in columns.items():
        if concept_lemma in activated_concepts:
            active_qualities = activated_qualities.get(concept_lemma, [])
            for quality_lemma, quality_info in list(neurons['quality_neurons'].items()):
                if quality_lemma in active_qualities:
                    if quality_info.get('first_activation', False):
                        quality_info['first_activation'] = False
                    else:
                        quality_info['permanence'] = quality_info['permanence'] ** 2
                else:
                    quality_info['permanence'] -= 1
                    if quality_info['permanence'] <= 0:
                        quality_neuron_id = quality_info['neuron_id']
                        if G.has_node(quality_neuron_id):
                            G.remove_node(quality_neuron_id)
                        del neurons['quality_neurons'][quality_lemma]

def update_permanence_values_instance_connections(activated_concepts, activated_instances):
    for concept_lemma, neurons in columns.items():
        if concept_lemma in activated_concepts:
            active_pairs = set()
            if concept_lemma in activated_instances:
                instance_neurons = activated_instances[concept_lemma]
                if len(instance_neurons) >= 2:
                    active_pairs = set(
                        tuple(sorted((instance_neurons[i], instance_neurons[j])))
                        for i in range(len(instance_neurons))
                        for j in range(i + 1, len(instance_neurons))
                    )
            for neuron_pair, connection_info in list(neurons['instance_connections'].items()):
                if neuron_pair in active_pairs:
                    if connection_info.get('first_activation', False):
                        connection_info['first_activation'] = False
                    else:
                        connection_info['permanence'] = connection_info['permanence'] ** 2
                else:
                    # Decrease permanence by 1
                    connection_info['permanence'] -= 1
                    if connection_info['permanence'] <= 0:
                        # Remove the connection
                        if G.has_edge(neuron_pair[0], neuron_pair[1]):
                            G.remove_edge(neuron_pair[0], neuron_pair[1])
                        del neurons['instance_connections'][neuron_pair]

def decrease_activation_trace_counters():
    for concept_lemma, neurons in columns.items():
        # Concept neuron
        if neurons['concept_activation_trace_counter'] > 0:
            neurons['concept_activation_trace_counter'] -= 1

        # Relation neurons
        for relation_info in neurons['relation_neurons'].values():
            if relation_info['activation_trace_counter'] > 0:
                relation_info['activation_trace_counter'] -= 1
            # Target connections
            if 'target_connections' in relation_info:
                for target_conn_info in relation_info['target_connections'].values():
                    if target_conn_info['activation_trace_counter'] > 0:
                        target_conn_info['activation_trace_counter'] -= 1

        # Quality neurons
        for quality_info in neurons['quality_neurons'].values():
            if quality_info['activation_trace_counter'] > 0:
                quality_info['activation_trace_counter'] -= 1

        # Instance connections
        for connection_info in neurons['instance_connections'].values():
            if connection_info['activation_trace_counter'] > 0:
                connection_info['activation_trace_counter'] -= 1

def draw_dependency_tree(sentence, tokens, columns, G):
    # Extract lemmas and positions
    lemmas = [token.lemma_ for token in tokens]
    pos_tags = [token.pos_ for token in tokens]
    dep_tags = [token.dep_ for token in tokens]
    tags = [token.tag_ for token in tokens]

    # Prepare the plot
    plt.figure(figsize=(12, 6))

    # Positions for nodes on x-axis
    x_positions = list(range(len(lemmas)))
    y_position = 0  # All nodes on the same vertical height

    # Determine node colors
    node_colors = []
    for idx, lemma in enumerate(lemmas):
        pos_tag = pos_tags[idx]
        if pos_tag in concept_pos_list:
            node_colors.append('blue')  # Concept node
        elif pos_tag in relation_pos_list:
            if lemma == 'have':
                node_colors.append('cyan')
            elif pos_tag == 'VERB':
                node_colors.append('green')
            elif pos_tag == 'ADP':
                node_colors.append('red')
            elif pos_tag == 'CONJ':
                node_colors.append('green')
            else:
                node_colors.append('gray')
        elif pos_tag in quality_pos_list:
            node_colors.append('turquoise')
        else:
            node_colors.append('gray')

    # Draw nodes as circles without edge colors
    node_size = 750 / (1.5 ** 2)
    plt.scatter(x_positions, [y_position]*len(lemmas), s=node_size, c=node_colors)

    # Draw lemma text inside the nodes with reduced font size
    for idx, lemma in enumerate(lemmas):
        plt.text(x_positions[idx], y_position, lemma, fontsize=8, ha='center', va='center', color='black')

    # Now determine the dependencies
    # We will store dependencies as tuples: (head_idx, dep_idx, dep_type, color)
    dependencies = []

    # For each concept lemma in the sentence
    for idx, lemma in enumerate(lemmas):
        token_pos = pos_tags[idx]
        if lemma not in columns:
            continue
        neurons = columns[lemma]
        # Process qualities
        for quality_lemma, quality_info in neurons.get('quality_neurons', {}).items():
            if quality_lemma in lemmas:
                quality_idx = lemmas.index(quality_lemma)
                # Check permanence value
                permanence = quality_info.get('permanence', 0)
                if permanence >= 3:
                    # Add dependency
                    dependencies.append((idx, quality_idx, 'quality', 'turquoise'))

        # Process relations
        for relation_lemma, relation_info in neurons.get('relation_neurons', {}).items():
            if relation_lemma in lemmas:
                relation_idx = lemmas.index(relation_lemma)
                # Check permanence value
                permanence = relation_info.get('permanence', 0)
                if permanence >= 3:
                    # Add dependency from concept to relation
                    # Determine color
                    pos = relation_info.get('pos', '')
                    if relation_lemma == 'have':
                        color = 'cyan'
                    elif pos == 'VERB':
                        color = 'green'
                    elif pos == 'ADP':
                        color = 'red'
                    elif pos == 'CONJ':
                        color = 'green'
                    else:
                        color = 'gray'
                    dependencies.append((idx, relation_idx, 'relation', color))
                    # Now check target connections
                    relation_neuron_id = relation_info['neuron_id']
                    if 'target_connections' in relation_info:
                        for target_neuron_id, target_conn_info in relation_info['target_connections'].items():
                            target_lemma = None
                            # Find the lemma corresponding to target_neuron_id
                            for i, l in enumerate(lemmas):
                                if l in columns and columns[l]['concept_neuron'] == target_neuron_id:
                                    target_lemma = l
                                    target_idx = i
                                    break
                            if target_lemma is not None:
                                # Check permanence
                                target_perm = target_conn_info.get('permanence', 0)
                                if target_perm >= 3:
                                    # Add dependency from relation to target concept
                                    dependencies.append((relation_idx, target_idx, 'relation_target', color))
        # Process definitions
        concept_neuron_id = neurons['concept_neuron']
        for other_idx, other_lemma in enumerate(lemmas):
            if other_lemma != lemma and other_lemma in columns:
                other_concept_neuron_id = columns[other_lemma]['concept_neuron']
                if G.has_edge(concept_neuron_id, other_concept_neuron_id):
                    edge_data = G.get_edge_data(concept_neuron_id, other_concept_neuron_id)
                    if edge_data.get('type') == 'definition':
                        dependencies.append((idx, other_idx, 'definition', 'darkblue'))

    # Now, draw the dependencies
    for head_idx, dep_idx, dep_type, color in dependencies:
        x1 = x_positions[head_idx]
        x2 = x_positions[dep_idx]
        xm = (x1 + x2) / 2
        width = abs(x2 - x1)
        if width == 0:
            width = 0.5  # Avoid zero width
        # Set height proportional to width to make arcs visible
        height = width / 2  # Increased height for better visibility
        # Draw an arc from x1 to x2
        arc = matplotlib.patches.Arc((xm, y_position), width=width, height=height, angle=0, theta1=0, theta2=180, color=color)
        plt.gca().add_patch(arc)

    plt.xlim(-1, len(lemmas))
    plt.ylim(-1, max(1.5, height))  # Adjusted ylim to ensure arcs are visible

    # Ensure the aspect ratio is equal to make nodes circular
    plt.gca().set_aspect('equal', adjustable='datalim')

    plt.axis('off')
    plt.show()

def process_sentences(sentences):
    global sentence_counter
    for sentence in sentences:
        # Print the sentence
        print(sentence)

        # Process sentence
        doc = nlp(sentence)
        tokens = [token for token in doc]
        # Prepare adjusted lists
        adjusted_lemmas = []
        adjusted_pos_tags = []
        adjusted_dep_tags = []
        adjusted_tags = []
        tokens_to_use = []

        for token in tokens:
            lemma = token.lemma_
            pos_tag = token.pos_
            dep_tag = token.dep_
            tag = token.tag_

            # Ignore "be" and "do" auxiliaries
            if dep_tag == 'aux' and lemma in ['be', 'do']:
                continue

            # Convert possessive clitic to "have"
            if tag == 'POS':
                lemma = 'have'
                pos_tag = 'VERB'
                dep_tag = 'aux'

            adjusted_lemmas.append(lemma)
            adjusted_pos_tags.append(pos_tag)
            adjusted_dep_tags.append(dep_tag)
            adjusted_tags.append(tag)
            tokens_to_use.append(token)

        # Increase the sentence counter
        sentence_counter += 1

        # Ensure all lemmas are in columns
        ensure_words_in_columns(adjusted_lemmas, tokens_to_use)

        # Collect qualities and initialize instance neurons
        activated_concepts, activated_instances, activated_qualities = collect_qualities_and_initialize_instances(
            adjusted_lemmas, adjusted_pos_tags, tokens_to_use)

        # Process relations
        activated_relations, activated_relation_targets = process_relations(
            adjusted_lemmas, adjusted_pos_tags, tokens_to_use, activated_instances)

        # Process definition connections
        process_definitions(tokens_to_use)

        # Update instance connections within the concept column
        update_instance_connections(activated_instances)

        # Update permanence values for concept neurons
        update_permanence_values_concept_neurons(activated_concepts)

        # Update permanence values for relation neurons and their target connections
        update_permanence_values_relation_neurons(
            activated_concepts, activated_relations, activated_relation_targets)

        # Update permanence values for quality neurons
        update_permanence_values_quality_neurons(activated_concepts, activated_qualities)

        # Update permanence values for instance connections
        update_permanence_values_instance_connections(activated_concepts, activated_instances)

        # Decrease activation trace counters
        decrease_activation_trace_counters()

        # Visualize the network
        visualize_network(G, columns)

        # Draw the dependency tree
        draw_dependency_tree(sentence, tokens_to_use, columns, G)

# Call the main processing function
process_sentences(sentences)

	---
	I get the error;

	line 444, in draw_networkx_nodes
    	raise nx.NetworkXError(f"Node {err} has no position.") from err
	networkx.exception.NetworkXError: Node 333 has no position.

	---
	FUTURE: the feature modifier lemmas are not being drawn in the dependency tree (eg "more")


